<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.82.1" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">
<title>nnUNet论文方法解析 | Joevaen</title>
<meta property="og:title" content="nnUNet论文方法解析" />
<meta property="og:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Joevaen.github.io/about/nnunet/nnunet2/" /><meta property="article:section" content="about" />

<meta property="article:modified_time" content="2021-07-13T15:31:45&#43;08:00" /><meta property="og:site_name" content="Joevaen" />

<meta itemprop="name" content="nnUNet论文方法解析">
<meta itemprop="description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》">
<meta itemprop="dateModified" content="2021-07-13T15:31:45&#43;08:00" />
<meta itemprop="wordCount" content="440">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="nnUNet论文方法解析"/>
<meta name="twitter:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》"/>



<link rel="preload" href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" as="style">
<link href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163836834-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163836834-2');
  gtag('config', 'UA-60127042-1');
</script>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="canonical" href="https://Joevaen.github.io/about/nnunet/nnunet2/">

<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@grpcio">
<meta name="twitter:creator" content="@grpcio">
<meta name="twitter:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta name="twitter:image:alt" content="Joevaen color logo">

<meta property="og:url" content="https://Joevaen.github.io/about/nnunet/nnunet2/">
<meta property="og:title" content="nnUNet论文方法解析">
<meta property="og:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》">
<meta property="og:type" content="article">
<meta property="og:site_name" content="Joevaen">
<meta property="og:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta property="og:image:type" content="image/png">
<meta property="og:image:alt" content="Joevaen color logo">
<meta property="og:locale" content="en_US">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  td-navbar-cover flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="672pt" height="436pt" viewBox="0 0 672 436"><g transform="translate(0.000000,436.000000) scale(0.100000,-0.100000)" fill="#000" stroke="none"><path d="M3066 3659c-376-41-707-264-894-602-35-63-50-101-45-113 4-12 3-15-5-10s-10 1-5-10c5-13 9-11 23 15 9 18 19 29 22 25 4-3 4 3 1 15-4 14-1 21 9 21 8 0 27 23 42 51 119 218 360 416 607 499 96 33 250 60 337 60 551 0 1009-390 1097-933 48-296-20-597-193-847-24-36-35-56-23-46 35 32 24 2-23-61-49-64-119-128-220-198-36-26-66-49-66-51 0-10 123 74 187 128 63 53 173 174 182 201 2 7 9 19 16 27s36 58 63 110c198 373 185 786-35 1159-25 42-60 95-79 118-42 51-45 68-4 22 68-76 190-285 190-326 0-7 5-12 11-10 6 1 14-4 17-12 2-8 0-11-5-8-6 4-8-4-5-19 3-18 7-22 12-13 6 8 12-2 19-29 6-23 15-59 21-80 31-121 29-332-5-516-9-44-13-81-11-83s16 47 31 109c26 106 27 122 22 268-4 119-12 179-32 260-56 225-191 440-375 598-70 59-261 182-284 182-6 0 25-21 69-47 44-25 105-65 135-89 62-48 149-134 135-134-6 0-32 23-60 51-67 67-224 171-325 215-78 34-249 83-294 85-21 0-21 1-1 9 15 7 8 9-30 10-27 0-72 2-1e2 4-27 2-86 0-129-5z"/><path d="M36e2 3580c8-5 20-10 25-10 6 0 3 5-5 10s-19 10-25 10c-5 0-3-5 5-10z"/><path d="M3519 3276c-69-18-136-62-211-138-191-194-325-488-288-630 12-45 45-88 67-88 18 0 16 19-2 27-24 9-45 57-45 103 0 121 141 397 271 531 111 115 241 189 303 173 1e2-25 76-265-64-626l-40-105-58-13c-79-17-211-65-309-110-197-92-359-233-429-375-35-71-39-87-39-154 0-65 4-83 28-123 61-104 188-133 299-70 70 41 176 152 251 265 67 102 188 333 243 464 37 89 39 92 79 102 43 11 84 36 74 45-3 3-24-2-47-11s-45-14-48-11 15 63 41 133c126 349 145 579 49 615-33 13-68 12-125-4zm-29-794c0-4-47-102-104-218-197-398-362-594-5e2-594-68 0-124 44-151 119-85 243 203 545 645 675 93 27 110 30 110 18z"/><path d="M2081 2794c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4291 2784c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2071 2754c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4302 2740c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2061 2714c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2052 2660c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2043 2585c0-22 2-30 4-17 2 12 2 30 0 40-3 9-5-1-4-23z"/><path d="M4252 2018c-15-37-19-55-11-63 7-7 8-4 3 9-6 15-4 17 6 11 9-5 11-4 6 3-4 7-3 12 3 12s7 7 4 17c-5 12-3 14 8 8 8-4 11-5 7 0-4 4-4 13 2 19 5 7 5 17 1 24s-15-8-29-40z"/><path d="M4205 1919c-3-4 2-6 10-5 21 3 28 13 10 13-9 0-18-4-20-8z"/><path d="M4024 1668l-19-23 23 19c12 11 22 21 22 23 0 8-8 2-26-19z"/><path d="M4015 1628l-40-43 43 40c39 36 47 45 39 45-2 0-21-19-42-42z"/><path d="M3969 1613c-13-16-12-17 4-4s21 21 13 21c-2 0-10-8-17-17z"/><path d="M3919 1543c-13-16-12-17 4-4 9 7 17 15 17 17 0 8-8 3-21-13z"/><path d="M3705 1428c-11-6-23-15-26-20-3-6-9-9-13-9-19 3-27 0-22-7 7-12-61-35-75-26-8 5-9 3-4-6 6-10 3-12-13-8-14 4-20 2-15-5 4-6-2-8-15-5-12 4-20 2-17-2 3-5 0-10-7-13-7-2-2-2 12 0 39 7 179 63 194 78 7 8 16 12 18 10 3-3 5 2 5 10s-1 15-1 15c-1 0-10-6-21-12z"/><path d="M3448 1313c7-3 16-2 19 1 4 3-2 6-13 5-11 0-14-3-6-6z"/><path d="M3355 13e2c-27-7-27-8-5-8 14 0 39 4 55 8 27 7 27 8 5 8-14 0-38-4-55-8z"/><path d="M3263 1283c9-2 25-2 35 0 9 3 1 5-18 5s-27-2-17-5z"/><path d="M1810 981c0-75-4-103-15-115-17-17-20-66-4-66 6 0 24 13 40 29 29 29 29 31 29 140v111h-25c-25 0-25 0-25-99z"/><path d="M2211 1062c-51-25-73-61-74-120-1-70 33-116 101-137 36-10 28 31-13 69-26 25-35 42-35 66s9 41 35 66c19 18 35 42 35 53 0 25-5 26-49 3z"/><path d="M2290 1060c0-12 16-36 35-54 26-25 35-42 35-66s-9-41-35-66c-41-38-49-79-12-69 66 20 99 65 99 135s-33 115-99 135c-19 5-23 2-23-15z"/><path d="M2692 1053l3-28h1e2 1e2l3 28 3 27h-106-106l3-27z"/><path d="M3150 1068c0-7 23-70 52-140 48-119 53-128 78-128s30 9 78 130c29 72 52 135 52 141s-12 9-27 7c-23-2-31-13-51-63-13-33-31-76-39-95l-14-35-25 59c-13 33-24 63-24 67s15 9 33 11c27 2 32 7 32 28 0 24-3 25-72 28-55 2-73 0-73-10z"/><path d="M3724 1003c-43-93-48-128-19-128 15 0 28 17 50 63l30 62 25-62c14-35 27-66 28-70 2-5-35-8-82-8-76 0-86-2-1e2-22-8-12-13-25-10-30 7-11 261-10 268 1 3 5-19 67-49 137-48 114-57 129-80 132s-29-4-61-75z"/><path d="M4187 1073c-4-3-7-14-7-24 0-25 28-31 125-27 79 3 80 3 83 31l3 27h-99c-54 0-102-3-105-7z"/><path d="M4670 1068c0-7 43-71 96-141 77-105 99-128 117-125 21 3 22 7 25 141l3 138-28-3c-28-3-28-4-33-86l-5-83-62 85c-50 69-68 86-88 86-14 0-25-6-25-12z"/><path d="M2694 955c-14-37 2-45 89-45 88 0 114 11 101 44-9 23-182 24-190 1z"/><path d="M4184 955c-3-8-3-22 0-30 5-13 22-15 98-13 92 3 93 3 93 28s-1 25-93 28c-76 2-93 0-98-13z"/><path d="M1675 890c-10-16 4-48 32-70 32-25 40-25 48 0 13 40-60 103-80 70z"/><path d="M4670 850c0-47 2-50 25-50s25 3 25 50-2 50-25 50-25-3-25-50z"/><path d="M2694 845c-3-8-3-22 0-30 5-13 23-15 103-13 97 3 98 3 98 28s-1 25-98 28c-80 2-98 0-103-13z"/><path d="M4184 845c-15-38 1-45 106-45h101l-3 28-3 27-98 3c-80 2-98 0-103-13z"/></g></svg></span><span class="text-uppercase font-weight-bold">Joevaen</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/about/" ><span>nnUNet</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/" ><span>MMDetection</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/showcase/" ><span>论文</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/" ><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/" ><span>C&#43;&#43;</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/yolov5/" ><span>Yolo V5</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">

</div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      
      <main role="main" class="td-main">
        












<section id="td-cover-block-0" class="row td-cover-block td-cover-block--height-sm js-td-cover td-overlay td-overlay--dark -bg-primary">
  <div class="container td-overlay__inner">
    <div class="row">
      <div class="col-12">
        <div class="text-center">
          
          
          <div class="pt-3 lead">
            
                <div class="text-left">
  <h1 class="display-1 mb-5">nnUNet论文方法解析</h1><h3 class="font-weight-light">《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》</h3>
  </div>

            
          </div>
        </div>
      </div>
    </div>
  </div>
  
</section>

<div class="container l-container--padded">
<div class="row">




  
    <div class="d-lg-none col-12">
      <div class="td-toc td-toc--inline">
  
      
        <a id="td-content__toc-link" class="collapsed" href="#td-content__toc" data-toggle="collapse" aria-controls="td-page-toc" aria-expanded="false" aria-label="Toggle toc navigation">
          <span class="lead">Contents<i class="fas fa-chevron-right ml-2"></i></span>
        </a>
        <div id="td-content__toc" class="collapse">
          <nav id="TableOfContents"></nav>
        </div>
        <button id="td-content__toc-link-expanded" href="#td-content__toc" class="btn btn-small ml-1 my-2 py-0 px-3" data-toggle="collapse" aria-controls="td-docs-toc" aria-expanded="true" aria-label="Toggle toc navigation">
        </button>
      
    </div>
  </div>

</div>
<div class="row">
<div class="col-12 col-lg-8">
<hr>
<p><font size=7><strong>写在前面</strong></font><font color=purple size=6><strong>（更新于07.27，由于最近在对推理部分进行加速，更新的有点慢，今天添加一些最近的新理解。紫色字体为更新的认识）</strong></font></p>
<p><strong>1.本文是对该论文涉及的<font face="华文琥珀" size=4  color=brown>Methods</font>进行解读和理解，参考上一篇的解读<a href="/about/nnunet/nnunet1/">论文主体解读</a></strong></p>
<p><strong>2.<font face="华文琥珀" size=4  color=red>红色是理解困难部分，正在进行结合附录、第一版nnunet论文、源码进行理解</font></strong></p>
<p><strong>3.<font face="华文琥珀" size=4  color=green>绿色是提到附录的部分</font></strong></p>
<p><strong>4.<font face="华文琥珀" size=4  color=orange>橙色是本文关键字</font></strong></p>
<p><strong>5.<font face="华文琥珀" size=4  color=blue>蓝色是抛出的问题</font></strong></p>
<hr>
<p><font size=7><strong>Methods</strong></font> (该部分提到的设计规则可以在<font color=green><strong>附录B</strong></font>中查看)</p>
<hr>
<ul>
<li><font size=6><strong>1.“数据指纹”</strong></font>
<ul>
<li>① 整个训练的第一步是将即将要训练的数据集的前景给抠出来（<strong>作者在这里说这个操作对于他们实验的数据来说没有影响，这可能是我在进行预处理时看到前后的图像尺寸没有变化的原因</strong>）；</li>
<li>② nnUNet根据这个得到的crop，来抓取它的相关参数和属性，来组成这个<strong>数据指纹</strong>，这些参数和属性包括：
<ul>
<li>Ⅰ. image_size： 每个空间维度上的体素的个数；</li>
<li>Ⅱ. image_spacing：每个体素的物理体积大小；</li>
<li>Ⅲ. 模态：头文件获取到的信息；</li>
<li>Ⅳ. 类别数：比如我是做单纯的二分类还多分类；</li>
<li>Ⅴ. <font color=red>此外，还包括在所有训练案例上计算得到平均值、标准差以及属于任何标签的体素的灰度值的99.5%和0.5%</font>
<ul>
<li></font><font color=purple size=3><strong>这部分灰度值的记录在plan.pkl中可以找到，简单的理解就是做归一化的一种行之有效的方式，通过找到训练集中每一套CT的前景像素值进行全局归一化。（具体我会继续研究）</strong></font></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><font size=6><strong>2.“管道指纹”</strong></font>
<ul>
<li>nnUNet将所有应该进行设计的参数缩减到必要的那几个，并且使用<font color=orange><strong>启发性规则</strong></font>对这些参数进行推理，该规则集成了很多行业内专业的知识，同时对上面提到的“数据指纹”和本地的硬件约束都能起到作用。</li>
<li>这些得到的<font color=orange><strong>推理参数</strong></font>被两种参数所完善：
<ul>
<li>第一种：<font color=orange><strong>蓝图参数</strong></font>，具有数据独立性，不同数据的该参数不同；</li>
<li>第二种：<font color=orange><strong>经验参数</strong></font>，训练期间被优化</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><font size=6><strong>3.“蓝图（计划）参数”</strong></font>
<ul>
<li><font color=orange><strong>一、结构模板</strong></font>：
<ul>
<li>nnUNet配置的这几个不同的网络结构都来自于相同的网络模板，这个模板和原始的UNet及其3D版本十分的相近。我们的理论认为，一个配置完好的nnUNet仍然很难被打败，我们提出的网络架构没有使用任何最近十分流行的网络结构，只是进行了一点相对前者那种大刀阔斧的改动来说<font color=orange><strong>小</strong></font>的多的<font color=orange><strong>改变</strong></font>。为了适应更大的patch_size，nnUNet把网络的batch_size设计的更小。事实上，大多数3D-Unet的batch_size都只有2（参见<font color=green><strong>附录E.1a</strong></font>）。Batch_normalization是用来干什么的呢？BN是用来加速训练同时保持训练稳定的，小的batch_size往往无法体现BN的价值。针对这种情况，<font color=orange><strong>我们所有的nnUNet的架构都使用了instance_normalization</strong></font>。更进一步的，我们用<font color=orange><strong>Leaky_Relu代替Relu（负轴上斜率为0.01）</strong></font>。</li>
</ul>
</li>
<li><font color=orange><strong>二、深监督训练</strong></font>：
<ul>
<li><font color=orange><strong>1.网络结构的一些改变</strong></font>：.除了解码阶段的最底下两层，我们给解码器的每一层都加了额外的损失，<font color=red>这样使得梯度信息能够更深层的注入网络，同时促进网络中所有层的训练</font>。所有的Unet网络在同一个像素层次上，都用的一样的操作，无论是编码区还是在解码区（都是一个卷积 + 一个instance_normalization + Leaky_Relu）。下采样是一个具有步幅长度的卷积，上采样是一次卷积的转置操作。为了平衡训练效果和显存消耗，最初的feature_map的大小被设定为32，如果要做一次下采样那么这个大小缩小一半，如果做上采样则会变成原来的两倍。为了限制最终生成的模型的大小，feature_map的数量也被做了一定的限制，比如，3D_Unet被限制在320而2D_Unet被限制在512。</li>
<li><font color=orange><strong>2.训练时间表</strong></font>：
<ul>
<li>Ⅰ.根据以往的一些经验，同时为了增加训练的时效性，我们用一轮大于 <font color=orange><strong>250的mini_batch</strong></font>，来把网络训练<font color=orange><strong>1000轮</strong></font>。随机梯度下降的方法采用<font color=orange><strong>μ = 0.99的nesterov的梯度下降</strong></font>，同时<font color=orange><strong>初始学习率为0.01.学习率的衰减遵循poly原则</strong></font>，即
$$
(1-epoch/epoch_{max})^{0.9},
$$
设计的损失函数是<font color=orange>**（交叉熵损失和 + dice_loss）**</font>，<font color=purple>**所以你得到的损失最优为-1，只要是在下降的就是正确的，损失的最小值没有意义**</font>。
为了得到每一个深度监督的输出，它对应的每一次分割后mask的ground_truth都用来计算损失，训练的对象就是这每一层的损失的和：
$$
L=w_1*L_1|w_2*L_2|w_3*L_3|&hellip;,
$$
根据这种方式，在每一次分辨率降低（下采样）时<font color=orange>**权重减半**</font>，就会使得：
$$
w_2=(1/2)*w_1;w_3=(1/4)*w_1,&hellip;
$$
同时将这些<font color=orange>**权重归一化**</font>到和为1。</li>
<li>Ⅱ. mini_batch的样本都是从训练案例之中随机选择的，通过采用<font color=orange><strong>过采样</strong></font>的方式来控制样本不均衡的带来的稳定性问题：<font color=red>Oversampling is implemented to ensure robust handling of class imbalances: 66.7% of samples are from random locations within the selected training case while 33.3% of patches are guaranteed to contain one of the foreground classes that are present in the selected training sample (randomly selected ).The number of foreground patches is rounded with a forced minimum of 1 (resulting in 1 random and 1 foreground patch with batch size 2).</font></li>
<li>Ⅲ.在训练的运行过程中其实是用到了很多的数据增强的方法，详情参照<font color=green><strong>附录D</strong></font>：
<ul>
<li>① 旋转</li>
<li>② 缩放</li>
<li>③ 高斯加噪</li>
<li>④ 高斯模糊</li>
<li>⑤ 亮度处理</li>
<li>⑥ 对比度处理</li>
<li>⑦ <font color=red>低分辨率仿真</font></li>
<li>⑧ <font color=red>Gamma（灰度系数）</font></li>
</ul>
</li>
</ul>
</li>
<li><font color=orange><strong>3.推理部分</strong></font>：
图像的推理预测是通过滑动窗口进行的，而窗口的大小等于训练时的patch_size，相邻的patch尺寸的预测（即滑动一次的两个块）具有一半的重叠比例。<font color=purple><strong>分割的准确率随着窗口边界的增大而降低。为了抑制拼接伪影，减少靠近边界位置的影响，采用高斯重要度加权，增加softmax聚合中中心体素的权重</strong></font>。<font color=red><strong>（我现在的理解是这部分与推理一并进行，一个个的patch类似于一yolo的滑动窗口，但是这部分的时间真的太慢了！！！）</strong></font><font color=purple><strong>通过沿所有轴进行镜像来增加测试时间,我将镜像去掉剪短了推理时间，一定程度上也损失了精度</strong></font>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><font size=6><strong>4.“推理参数”</strong></font>
<ul>
<li><font color=orange><strong>1.色彩强度（灰度）归一化</strong></font>：
nnUNet支持两种灰度归一化的方式：
* ① 对于除了CT之外的其他所有模态的归一化方法是z-scoring：在训练和推理过程中，先对每幅图像分别进行减去他们的强度平均值，同时除以他们的标准差来进行归一化。<font color=red>If cropping resulted inan average size decrease of 25% or more, a mask for central non-zero voxels is created and the normalization is applied within that mask only, ignoring the surrounding zero voxels</font>（如果裁剪导致平均尺寸减少25%或更多，则创建中心体素不是0的mask<font color=red><strong>（mask的大小？）</strong></font>，并只在该掩模内应用标准化，忽略周围的零体素）
* ② 那么对于CT图的计算，就换用了另外一种方法：因为CT图的每一层各个像素的灰度值是一个定量并且反映的是该切片上的一些物理属性，因此这种有利于使用全局归一化的方式，来应用到所有的图片上面。<font color=red>To this end, nnU-Net uses the 0.5 and 99.5 percentiles of the foreground voxels for clipping as well as the global foreground mean a standard deviation for normalization on all images. </font>。</li>
<li><font color=orange><strong>2.重采样</strong></font>：
       在一些数据集中，尤其是医学图像中，<font color=purple><strong>voxel_spacing</strong></font>（体素块间距）是一个非常重要的属性，而且这个属性通常是很多样的（有些厚有些薄）。但是我们卷积处理图像的时候是在处理一个数组或者tensor，是没有这个voxel_spacing的信息的，为了适应这种spacing很多样的特征，就需要用到一些插值算法，比如说三阶样条插值、线性插值、临近插值等。对于具备各向异性的图片（spacing最大的那个轴的spacing / spacing最小的那个轴的spacing 是大于3的）来说，平面内（x轴, y轴）的插值采用的是<font color=purple><strong>三阶样条插值</strong></font>，而平面外（z轴）的插值用的是<font color=purple><strong>最邻近插值</strong></font>。对z轴进行不同的插值方法会抑制重采样的伪影，比如如果voxel_spacing很大，那两个相邻切片的边缘的变化就通常很大。
       对于分割图像的重采样其实是通过将他们转换成<font color=purple><strong>独热编码</strong></font>。每一个通道都要用一次<font color=purple><strong>线性插值</strong></font>，然后再用<font color=purple><strong>argmax</strong></font>对分割图片进行恢复。再者，对于各项异性图片来说，是在低分辨率的那几层进行<font color=purple><strong>最邻近插值</strong></font>。
<ul>
<li><font color=purple size=4><strong>2.1 谈一下推理部分的重采样到底是如何进行的，结合第三篇的B.2的4</strong></font>
       这里说起来有点绕，我们这样做分析：
<ul>
<li>采样的对象：
<ul>
<li>① 训练时输入的标签和推理时输出的标签：独热编码后进行线性插值，</li>
<li>② 训练时输入的数据和推理时输入的数据：
<ul>
<li>Ⅰ. 各向同性则不进行z轴的插值，xy使用临近插值；</li>
<li>Ⅱ. 各向异性则对xy进行最邻近插值，对z进行三阶样条插值。</li>
</ul>
</li>
</ul>
</li>
<li>显然，和我上面说的有出入，但这是代码里的呈现，所以我怀疑论文里面的部分写反了。</li>
</ul>
</li>
</ul>
</li>
<li><font color=orange><strong>3.Target-Spacing(<font color=purple><strong>看上面部分</strong></font>)</strong></font>：
       目标间距:选择的目标间距是一个关键参数。更大的间隔导致更小的图像，从而丢失细节，而更小的间隔导致更大的图像，阻止网络积累足够的上下文信息，因为patch的大小受到给定GPU内存预算的限制。虽然3D U-Net级联部分解决了这个问题，但仍然需要合理的目标间距来实现低分辨率和全分辨率。对于3D全分辨率U-Net, nnU-Net使用每个轴独立计算的训练用例中间隔的中值作为默认目标间距。对于各向异性数据集，这种默认会导致严重的插值伪影，或者由于训练数据在分辨率上的巨大差异而导致大量信息丢失。因此，当体素和间距各向异性(即最低间距轴与最高间距轴之比)均大于3时，在训练案例中，选择最低分辨率轴的目标间距为间距的第10个百分位。对于2D U-Net, nnU-Net通常在分辨率最高的两个轴上运行。如果三个轴都是各向同性的，则利用两个后向轴进行切片提取。目标间距是训练案例的中值间距(每个轴独立计算)。对于基于切片的处理，不需要沿面外轴重新采样。
<ul>
<li><font color=purple size=4><strong>3.1 谈一下推理部分的target_spacing到底是如何选择的，结合第三篇的B.2的4</strong></font>
       以我自己训练的50套CT数据为例，这些CT原来的spacing有些一样，有些不一样。我将训练生成的plan.pkl文件进行了具体的查看，里面汇总了训练数据集所有的相关属性，包括他们的模态、尺寸、三个方向上的spacing等。我选取了一个尺寸为（ 122, 512, 512），spacing为（[3.        , 0.64999998, 0.64999998]）的数据进行推理测试。推理时，我将采样后的结果打印，为{&lsquo;spacing&rsquo;: ([1.25      , 0.74121118, 0.74121118]), &lsquo;data.shape : ( 293, 449, 449)}。
       按照作者对该算法的描述，因为3 &gt; 0.64x3，所以这个数据属于各向异性的数据，那么z轴的插值应当取排列后的10%的数。我对此进行了计算，50个数据中，spacing为1.25的有11个，其余都比1.25大，显然我的target_spacing就应该是1.25(作者诚不欺我)。而当我计算x轴和y轴上的target_spacing时，发现并不是10%的那个值，反而是中位数。说明一点，无论我待测的CT是各向同性还是各向异性的，x，y轴上的target_spacing应当是所有排列以后的中位数（median），而z轴的target_spacing则因性而异，各向同性则为中位数，各项异性则为10%的那个数。
       因为仍然怀疑正确性，我找了另一个各向同性的CT来进行测试。但是我发现，target_spacing仍然和上面一样，于是我检查代码，发现在推理的时候，target_spacing是从训练时生成的plan.pkl中读取的，也就是一个固定的值，无关异性还是同性。这个问题我会在github上做一个提问。
       让我做一下思考：
       ①我的推理结果和我的训练数据息息相关，假如我的训练数据的spacing都相等，那么我的推理数据也会以同样的spacing进行插值。
       ②我的待推理数据与他自身属性之间的关系，反而没有它和训练数据属性之间的关系大？无论你原来什么样子，你的spacing终究会和输进网络训练的那个东西的spacing一样。这是否意味着，训练集的质量相当重要？如果我的训练数据的源头一致，那么这个源头产生的测试集，也会具有更好的精准度？
       ③z轴越大插值时间越久，CT的原spacing越小需要插值的次数就越多。</li>
</ul>
</li>
<li><font color=orange><strong>4.网络拓扑、patch_size、batch_size的自适应性</strong></font>：
<ul>
<li>① 寻找一个合适网络拓扑结构对于得到一个优秀的分割表现相当的重要。在一定的GPU显存的限制下，nnUNet优先选择一个大的patch_size，更大的patch_size会获取到更多的上下文的信息，这显然能提高模型的表现。但是，相对应的代价就是要减小batch_size（同时很大肯定占显存），而减小batch_size就会使反向传播的梯度下降变得不稳定（有时候学的好有时候学的坏，因为输入的不同batch的数据差距大）。为了提高训练的稳定性，我们提出一个值为2的mini_batch同时在网络训练时加入一个大的动量项（<strong>蓝图参数</strong>里有）。</li>
<li>② 图像的spacing也被作为自适应的一部分去考虑：
<font color=red>下采样的操作很可能只在某个特定的轴上进行操作，3D_Unet的卷积核可能对特定的平面进行操作（伪2D）。所有U-Net配置的网络拓扑都是根据重新采样后图像尺寸的中值以及重新采样到的目标间距来选择的。</font><font color=green><strong>附录E.1</strong></font>有一个流程图展示了这个自适应的流程。接下来会探讨更多的网络结构模板的自适应的细节，通常都不会以昂贵的计算力为代价。因为GPU的内存消耗估计是基于feature_map的大小，而这个适应过程和feature_map的关系很小，所以不需要GPU参与这个进程。</li>
</ul>
</li>
<li><font color=orange><strong>5.初始化</strong></font>：
       patch_size被初始化成重采样以后的图像尺寸的中位数，如果这个patch_size不能被$$2^{n_d}（下采样的次数）$$整除，那么将会对图像进行填充。</li>
<li><font color=orange><strong>6.拓扑结构</strong></font>：
<ul>
<li>① 网络结构通过确定每根轴上的下采样次数，而这个下采样的次数，又取决于patch_size的大小和voxel_spacing的大小。</li>
<li>② 下采样会一直进行，除非上一个下采样会把feature_map的尺寸减小到4个体素以下，或者特征图的spacing是各向异性的。</li>
<li>③ 下采样策略是被体素间隔（voxel_spacing）来确定的，<font color=red><strong>high resolution axes are downsampled separately until their resolution is within factor 2 of the lower resolution axis（高分辨率轴分别向下采样，直到它们的分辨率在低分辨率轴的2倍之内？）</strong></font><font color=blue><strong>【低分辨率是如何设计好的？】</strong></font>，随后所有的轴同时进行下采样。所有轴的下采样是分别中止的，分别终止于他们的特征图的尺寸达到某个它们各自的限制条件。</li>
<li>④ 3D_Unet和2D_Unet默认的卷积核的大小分别是3x3x3和3x3。<font color=red><strong>If there is an initial resolution discrepancy between axes (defined as a spacing ratio larger than 2), the kernel size for the out-of-plane axis is set to 1 until the resolutions are within a factor of 2. Note that the convolutional kernel size then remains at 3 for all axes.</strong></font> <font color=blue><strong>【这里的轴到到底是什么意思？通道上也采样？卷积核为1尺寸不是不变化？】</strong></font></li>
</ul>
</li>
<li><font color=orange><strong>7.对GPU内存限制的自适应</strong></font>：
       patch_size的大小是根据GPU 的大小尽可能大的进行设定的。由于在重新采样后，patch_size的大小被初始化为图像尺寸的中值大小，所以对于大多数数据集来说，patch_size最初太大了，无法适应GPU。nnU-Net根据网络feature_map的大小来估计给定架构的内存消耗，并将其与已知内存消耗的参考值进行比较。然后在迭代过程中减少patch_size的大小，同时在每个步骤中相应地更新架构配置，直到达到所需的预算(参见<font color=green><strong>附录E.1</strong></font>)。缩小patch_size的大小总是应用在数据相对于图像中值形状的最大轴上。 <font color=red><strong>The reduction in one step amounts to 2^(n_d) voxels of that axis, where nd is the number of downsampling operations.</strong></font></li>
<li><font color=orange><strong>8.batch_size</strong></font>：
       最后一步是配置batch_size。如果执行了减少patch_size大小的操作，则批大小设置为2。否则，剩余的GPU内存空间将会用来增加batch_size，直到GPU被充分利用为止。为了防止过拟合，对batch_size进行了限制，使mini_batch中的体素总数不超过所有训练案例体素总数的5%。<font color=green><strong>附录C1&amp;C2</strong></font>给出了生成U-Net架构的例子。</li>
<li><font color=orange><strong>9.3D_Unet_cascade是如何配置的</strong></font>：
<ul>
<li>
<p>① 在下采样数据上进行模型分割增大了与图像相关的patch_size，这样可以使网络获取更多的上下文信息，但这是以牺牲模型在一些细节或者纹理上的分割表现为代价的。假设有一个显存无穷大的GPU，那么最好的patch_size应当覆盖整个图像大小。</p>
</li>
<li>
<p>② 3D_UNet_cascade是如何模拟patch_size无穷大这个过程：</p>
<ul>
<li>Ⅰ 先用3D_Unet在下采样数据集上跑一个模型；</li>
<li>Ⅱ 再用一个全像素的3D_Unet来修正前者的分割效果；</li>
</ul>
<p>利用这种方法，低分辨率下采样层会获得更大的上下文信息，以此来生成它的分割输出，这个输出作为一个新的通道加入到下一个模块的输入中。</p>
</li>
<li>
<p>③ 这个级联什么时候会被使用：</p>
<ul>
<li>Ⅰ仅仅当3d_full_resolution的patch_size小于图像尺寸中值的12.5%使会使用级联，如果这个事件发生了，那么下采样数据的target_spacing和低分辨率部分的网络架构将会再一个迭代过程中被共同配置。</li>
<li>Ⅱ 这个迭代过程是这样的：初始化的target_spacing是全像素的target_spacing，为了让patch_size占据整个图的合适的一部分，在更新网络结构的过程中，逐渐每次增加1%的target_spacing，直到网络拓扑的patch_size超过目前图像尺寸中值的25%，如果目前的spacing是各向异性的（ <font color=red>factor 2 difference between lowest and highest resolution axis</font>）<font color=blue><strong>【究竟医学图像的各向异性如何解释？】</strong></font>，那就只有更高分辨率的轴的spacing会被增大。</li>
<li>Ⅲ 第二个3D_Unet的配置和独立的3D_Unet的上面的配置是一样的，除了第一个UNet的上采样的分割图与它的输入是condatenated。<font color=green><strong>附录E.1b</strong></font>就是这个优化过程的描述。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><font size=6><strong>5.“经验参数”</strong></font> ：
<ul>
<li>① <font color=orange><strong>推理模式</strong></font>：
       nnUNet会在推理时根据训练集中的验证dice表现来自动选择配置——是选择单个推理方式还是选择一起推理方式。
       四种单个推理模式：2D、3D_full resolution、3Dliangl_lower resolution &amp;the full resolution of the cascade。
       合作推理模式：四个模式中两两进行组合来实现合作推理。
       <font color=red><strong>通过平均sofmax概率对模型进行综合。</strong></font></li>
<li>② <font color=orange><strong>后处理</strong></font>：
       <font color=red><strong>Connected Components算法（连通分支算法）</strong></font>在医学图像领域非常常用，尤其是在器官分割中，经常用于去除最大的连接组件，从而来消除假阳性。nnU-Net同样用到这个方法，并自动对交叉验证中抑制较小组件的效果进行测试：首先所有的前景会被当做一个组件（多类别的1、2、3都被当做1），如果对除最大区域以外的所有分支的抑制提高了前景的平均dice而没有减小任何类别的dice，这一步将会被选择作为第一步的后处理步骤。最终，nnUNet会依靠这个步骤的表现来衡量要不要把相同的步骤用于不同的类别上。</li>
</ul>
</li>
</ul>



      </main>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://github.com/Joevaen/Joevaen.github.io">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 JV
          
        </small>
        
        
      </div>
    </div>
    <div class="row text-center text-white small">
      <div class="col-12 text-center py-2 order-sm-2">
        <a href="https://www.linuxfoundation.org/terms" target="_blank" rel="noopener">Terms</a> |
        <a href="https://www.linuxfoundation.org/privacy" target="_blank" rel="noopener">Privacy</a> |
        <a href="https://www.linuxfoundation.org/trademark-usage" target="_blank" rel="noopener">Trademarks</a> |
        <a href="https://github.com/grpc/grpc.io/blob/main/LICENSE" target="_blank" rel="noopener">License</a> |
        <a href="/about/">About</a>
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>











<script src="/js/main.min.d862e8117d34e02dff1772aeeb47caf7ed851d0f8a3a5345f7a32ccfb2f6b87f.js" integrity="sha256-2GLoEX004C3/F3Ku60fK9&#43;2FHQ&#43;KOlNF96Msz7L2uH8=" crossorigin="anonymous"></script>




  </body>
</html>