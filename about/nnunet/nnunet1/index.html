<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.82.1" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">
<title>nnUNet论文主体解析 | gRPC</title>
<meta property="og:title" content="nnUNet论文主体解析" />
<meta property="og:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Joevaen.github.io/about/nnunet/nnunet1/" /><meta property="article:section" content="about" />

<meta property="og:site_name" content="gRPC" />

<meta itemprop="name" content="nnUNet论文主体解析">
<meta itemprop="description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》">

<meta itemprop="wordCount" content="206">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="nnUNet论文主体解析"/>
<meta name="twitter:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》"/>



<link rel="preload" href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" as="style">
<link href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163836834-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163836834-2');
  gtag('config', 'UA-60127042-1');
</script>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="canonical" href="https://Joevaen.github.io/about/nnunet/nnunet1/">

<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@grpcio">
<meta name="twitter:creator" content="@grpcio">
<meta name="twitter:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta name="twitter:image:alt" content="gRPC color logo">

<meta property="og:url" content="https://Joevaen.github.io/about/nnunet/nnunet1/">
<meta property="og:title" content="nnUNet论文主体解析">
<meta property="og:description" content="《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》">
<meta property="og:type" content="article">
<meta property="og:site_name" content="gRPC">
<meta property="og:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta property="og:image:type" content="image/png">
<meta property="og:image:alt" content="gRPC color logo">
<meta property="og:locale" content="en_US">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  td-navbar-cover flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="672pt" height="436pt" viewBox="0 0 672 436"><g transform="translate(0.000000,436.000000) scale(0.100000,-0.100000)" fill="#000" stroke="none"><path d="M3066 3659c-376-41-707-264-894-602-35-63-50-101-45-113 4-12 3-15-5-10s-10 1-5-10c5-13 9-11 23 15 9 18 19 29 22 25 4-3 4 3 1 15-4 14-1 21 9 21 8 0 27 23 42 51 119 218 360 416 607 499 96 33 250 60 337 60 551 0 1009-390 1097-933 48-296-20-597-193-847-24-36-35-56-23-46 35 32 24 2-23-61-49-64-119-128-220-198-36-26-66-49-66-51 0-10 123 74 187 128 63 53 173 174 182 201 2 7 9 19 16 27s36 58 63 110c198 373 185 786-35 1159-25 42-60 95-79 118-42 51-45 68-4 22 68-76 190-285 190-326 0-7 5-12 11-10 6 1 14-4 17-12 2-8 0-11-5-8-6 4-8-4-5-19 3-18 7-22 12-13 6 8 12-2 19-29 6-23 15-59 21-80 31-121 29-332-5-516-9-44-13-81-11-83s16 47 31 109c26 106 27 122 22 268-4 119-12 179-32 260-56 225-191 440-375 598-70 59-261 182-284 182-6 0 25-21 69-47 44-25 105-65 135-89 62-48 149-134 135-134-6 0-32 23-60 51-67 67-224 171-325 215-78 34-249 83-294 85-21 0-21 1-1 9 15 7 8 9-30 10-27 0-72 2-1e2 4-27 2-86 0-129-5z"/><path d="M36e2 3580c8-5 20-10 25-10 6 0 3 5-5 10s-19 10-25 10c-5 0-3-5 5-10z"/><path d="M3519 3276c-69-18-136-62-211-138-191-194-325-488-288-630 12-45 45-88 67-88 18 0 16 19-2 27-24 9-45 57-45 103 0 121 141 397 271 531 111 115 241 189 303 173 1e2-25 76-265-64-626l-40-105-58-13c-79-17-211-65-309-110-197-92-359-233-429-375-35-71-39-87-39-154 0-65 4-83 28-123 61-104 188-133 299-70 70 41 176 152 251 265 67 102 188 333 243 464 37 89 39 92 79 102 43 11 84 36 74 45-3 3-24-2-47-11s-45-14-48-11 15 63 41 133c126 349 145 579 49 615-33 13-68 12-125-4zm-29-794c0-4-47-102-104-218-197-398-362-594-5e2-594-68 0-124 44-151 119-85 243 203 545 645 675 93 27 110 30 110 18z"/><path d="M2081 2794c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4291 2784c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2071 2754c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4302 2740c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2061 2714c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2052 2660c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2043 2585c0-22 2-30 4-17 2 12 2 30 0 40-3 9-5-1-4-23z"/><path d="M4252 2018c-15-37-19-55-11-63 7-7 8-4 3 9-6 15-4 17 6 11 9-5 11-4 6 3-4 7-3 12 3 12s7 7 4 17c-5 12-3 14 8 8 8-4 11-5 7 0-4 4-4 13 2 19 5 7 5 17 1 24s-15-8-29-40z"/><path d="M4205 1919c-3-4 2-6 10-5 21 3 28 13 10 13-9 0-18-4-20-8z"/><path d="M4024 1668l-19-23 23 19c12 11 22 21 22 23 0 8-8 2-26-19z"/><path d="M4015 1628l-40-43 43 40c39 36 47 45 39 45-2 0-21-19-42-42z"/><path d="M3969 1613c-13-16-12-17 4-4s21 21 13 21c-2 0-10-8-17-17z"/><path d="M3919 1543c-13-16-12-17 4-4 9 7 17 15 17 17 0 8-8 3-21-13z"/><path d="M3705 1428c-11-6-23-15-26-20-3-6-9-9-13-9-19 3-27 0-22-7 7-12-61-35-75-26-8 5-9 3-4-6 6-10 3-12-13-8-14 4-20 2-15-5 4-6-2-8-15-5-12 4-20 2-17-2 3-5 0-10-7-13-7-2-2-2 12 0 39 7 179 63 194 78 7 8 16 12 18 10 3-3 5 2 5 10s-1 15-1 15c-1 0-10-6-21-12z"/><path d="M3448 1313c7-3 16-2 19 1 4 3-2 6-13 5-11 0-14-3-6-6z"/><path d="M3355 13e2c-27-7-27-8-5-8 14 0 39 4 55 8 27 7 27 8 5 8-14 0-38-4-55-8z"/><path d="M3263 1283c9-2 25-2 35 0 9 3 1 5-18 5s-27-2-17-5z"/><path d="M1810 981c0-75-4-103-15-115-17-17-20-66-4-66 6 0 24 13 40 29 29 29 29 31 29 140v111h-25c-25 0-25 0-25-99z"/><path d="M2211 1062c-51-25-73-61-74-120-1-70 33-116 101-137 36-10 28 31-13 69-26 25-35 42-35 66s9 41 35 66c19 18 35 42 35 53 0 25-5 26-49 3z"/><path d="M2290 1060c0-12 16-36 35-54 26-25 35-42 35-66s-9-41-35-66c-41-38-49-79-12-69 66 20 99 65 99 135s-33 115-99 135c-19 5-23 2-23-15z"/><path d="M2692 1053l3-28h1e2 1e2l3 28 3 27h-106-106l3-27z"/><path d="M3150 1068c0-7 23-70 52-140 48-119 53-128 78-128s30 9 78 130c29 72 52 135 52 141s-12 9-27 7c-23-2-31-13-51-63-13-33-31-76-39-95l-14-35-25 59c-13 33-24 63-24 67s15 9 33 11c27 2 32 7 32 28 0 24-3 25-72 28-55 2-73 0-73-10z"/><path d="M3724 1003c-43-93-48-128-19-128 15 0 28 17 50 63l30 62 25-62c14-35 27-66 28-70 2-5-35-8-82-8-76 0-86-2-1e2-22-8-12-13-25-10-30 7-11 261-10 268 1 3 5-19 67-49 137-48 114-57 129-80 132s-29-4-61-75z"/><path d="M4187 1073c-4-3-7-14-7-24 0-25 28-31 125-27 79 3 80 3 83 31l3 27h-99c-54 0-102-3-105-7z"/><path d="M4670 1068c0-7 43-71 96-141 77-105 99-128 117-125 21 3 22 7 25 141l3 138-28-3c-28-3-28-4-33-86l-5-83-62 85c-50 69-68 86-88 86-14 0-25-6-25-12z"/><path d="M2694 955c-14-37 2-45 89-45 88 0 114 11 101 44-9 23-182 24-190 1z"/><path d="M4184 955c-3-8-3-22 0-30 5-13 22-15 98-13 92 3 93 3 93 28s-1 25-93 28c-76 2-93 0-98-13z"/><path d="M1675 890c-10-16 4-48 32-70 32-25 40-25 48 0 13 40-60 103-80 70z"/><path d="M4670 850c0-47 2-50 25-50s25 3 25 50-2 50-25 50-25-3-25-50z"/><path d="M2694 845c-3-8-3-22 0-30 5-13 23-15 103-13 97 3 98 3 98 28s-1 25-98 28c-80 2-98 0-103-13z"/><path d="M4184 845c-15-38 1-45 106-45h101l-3 28-3 27-98 3c-80 2-98 0-103-13z"/></g></svg></span><span class="text-uppercase font-weight-bold">gRPC</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/about/" ><span>nnUNet</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/" ><span>MMDetection</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/showcase/" ><span>论文</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/" ><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/" ><span>C&#43;&#43;</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">

</div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      
      <main role="main" class="td-main">
        












<section id="td-cover-block-0" class="row td-cover-block td-cover-block--height-sm js-td-cover td-overlay td-overlay--dark -bg-primary">
  <div class="container td-overlay__inner">
    <div class="row">
      <div class="col-12">
        <div class="text-center">
          
          
          <div class="pt-3 lead">
            
                <div class="text-left">
  <h1 class="display-1 mb-5">nnUNet论文主体解析</h1><h3 class="font-weight-light">《Automated Design of Deep Learning Methods for Biomedical Image Segmentation》</h3>
  </div>

            
          </div>
        </div>
      </div>
    </div>
  </div>
  
</section>

<div class="container l-container--padded">
<div class="row">




  
    <div class="d-lg-none col-12">
      <div class="td-toc td-toc--inline">
  
      
        <a id="td-content__toc-link" class="collapsed" href="#td-content__toc" data-toggle="collapse" aria-controls="td-page-toc" aria-expanded="false" aria-label="Toggle toc navigation">
          <span class="lead">Contents<i class="fas fa-chevron-right ml-2"></i></span>
        </a>
        <div id="td-content__toc" class="collapse">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#1介绍">1.介绍</a></li>
    <li><a href="#2结果">2.结果</a>
      <ul>
        <li><a href="#1nnunet可以自动适应任何新的数据">1.nnUNet可以自动适应任何新的数据</a></li>
        <li><a href="#2nnunet极佳地掌握了目标标签的结构和数据图片的属性">2.nnUNet极佳地掌握了目标标签的结构和数据图片的属性</a></li>
        <li><a href="#3nnunet在多个不同任务中的表现都要比一些刻意设计的管道要好">3.nnUNet在多个不同任务中的表现，都要比一些刻意设计的“管道”要好</a></li>
        <li><a href="#4管道配置对结果的影响要比网络结构变化对结果的影响大">4.“管道”配置对结果的影响要比网络结构变化对结果的影响大</a></li>
        <li><a href="#5不同的数据集需要不同管道配置">5.不同的数据集需要不同“管道”配置</a></li>
        <li><a href="#6多任务提升了决策方案的鲁棒性">6.多任务提升了决策（方案）的鲁棒性</a></li>
      </ul>
    </li>
    <li><a href="#3讨论">3.讨论</a></li>
  </ul>
</nav>
        </div>
        <button id="td-content__toc-link-expanded" href="#td-content__toc" class="btn btn-small ml-1 my-2 py-0 px-3" data-toggle="collapse" aria-controls="td-docs-toc" aria-expanded="true" aria-label="Toggle toc navigation">
        </button>
      
    </div>
  </div>

</div>
<div class="row">
<div class="col-12 col-lg-8">
<h1 id="讲在前面">讲在前面</h1>
<ul>
<li>一.更新于2020.07.10，读完整篇nnUNet的20年4月份发表的论文，在更深的理解之后，做一些修正；</li>
<li>二.我设计了几种字体颜色用于更加醒目地表现关键的思想和主题：
<ul>
<li><strong><font color=red>红色表示尚未理解透彻的一些概念</font></strong></li>
<li><strong><font color=blue>蓝色表示对原来的理解做的一些修改或补充</font></strong></li>
<li><strong><font color=green>绿色表示此处需要参考的论文其他部分</font></strong></li>
<li><strong><font color=orange>橙色表示本文的重要关键字</font></strong></li>
<li><strong><del>我会用删除线将自己曾经不到位的理解进行删除</del></strong></li>
</ul>
</li>
<li>三.<strong><font color=orange>一些关键字总结</font></strong>（对于理解整篇论文都相当有必要，按照文中意思进行的理解）：
<ul>
<li>1.<font color=orange><strong>实验者（ end-users）：</strong></font>医学图像分割的实践者和研究人员</li>
<li>2.<font color=orange><strong>病例（cases）：</strong></font>在本文包括之后的文章，我都会把case翻译成病例，一个病例指的就是一套CT。</li>
<li>3.<font color=orange><strong>数据指纹（data fingerprint）：</strong></font>数据属性的集合</li>
<li>4.<font color=orange><strong>管道指纹（pipeline fingerprint）：</strong></font>从数据到模型这个过程中所经历的参数和设计方法；</li>
<li>5.<font color=orange><strong>启发性规则（ heuristic rules）：</strong></font>一些医学图像处理的专业知识的集成和经验的总结，当你想通过数据指纹计算出所需要的管道指纹时，正是依靠的这些算法和方法；</li>
</ul>
</li>
</ul>
<h1 id="摘要">摘要</h1>
<p>        医学图像作为科学研究的重要驱动和医疗界的核心元素，最近被深度学习深深的刺激。而语义分割使得很多领域的3D图像分析和应用成为可能，而且针对不同的任务的不同的方法都是不简单的，而且相当依赖数据集的质量和硬件条件。我们提出<strong>nnUNet</strong>，一种浓缩了该领域的大部分知识，并且具备自动为不同任务设计不同训练方案的框架（源自一个最基本框架的衍生）。不需要人工进行调参，nnUNet在19个国际竞赛中取得最优结果，而且在49个任务的大多数中，都达到了SOTA级别。结果证明了这种深度学习的自适应机制具有巨大的潜力。我们将该框架开源，使得它变得开箱即用，只是为了使得这种SOTA级别的方法更加平民化，从而来促进自动化框架设计的发展。</p>
<hr>
<h1 id="论文内容">论文内容</h1>
<h2 id="1介绍">1.介绍</h2>
<p>        语义分割将医学图像转化成有意义的空间结构信息，因此对医学的发展有相当必要的作用，尤其对于很多临床应用来说是十分必要的元素，比如一些与人工智能相关的诊断系统、治疗计划、术中协助以及肿瘤的生长监视。自动的医学图像分割是如此的火热，以至于国际上的图像分析赛事有将近70%是有关于医学图像的。
        尽管最近基于深度学习的图像分割方法取得了重大的进展，但是对于<del>终端用户</del><font color=orange><strong>实验者</strong></font> 来说，这些方法都有特定问题上的局限性。某个特定的分割任务的设计往往需要大量的实验时间和很高的训练水平，往往一个很小的错误就会导致一个巨大的效果落差。尤其是在3D医学图像领域，遇到图片质量、图片模态、图片大小、体素大小、类别比率都有差别的这种挑战，这时整个方法的设计可能就会非常笨重和繁琐，因此适应于其他数据或任务的好的优化方法（参数）可能对你手头的任务和数据并没有太大作用。大部分专业的人士做的事情，都是从网络结构的优化设计，到数据增强和后处理。每一个子组件都由一些基础的超参数所决定，例如learn       rate、batch size、或者<font color=red>class sample</font>。
        还有另一个问题就是训练和推理对于硬件的需求。算法的优化往往依赖于对高纬度的超参数的需求，由于训练<del>范例</del> <font color=orange><strong>病例（cases）：</strong></font>的增多也会使得计算资源几何式增长。导致的结果就是，实验者通常都会经过千百次的实验之后得到一个不好的结果，其实这是和他们自己的任务和数据集特性相关的，而且这种错误很难说明也不好复制，他们便不可避免的得到了并不是最佳的方法或者并不适用于其他数据集的研究成果。
        有篇论文对这些情况做了一些解释反而使情况变得更加复杂，这个研究的很大一部分对于非专业人士来说是难以理解的，甚至对于专业人士来说也很难评估。2015年共有将近12000片论文引用了UNet用来作为医学图像分割的工具，并且很多对UNet做了拓展和改进。我们假设最原始的UNet仍然是足够强大的，只要它具备足够合适的设计。
        最后，我们提出了nnUNet，最终这个网络使得医学研究的实际应用成为了可能，至于这个框架为什么可以适应任意的数据集并且能够开箱即用，主要缘于以下两点:</p>
<ul>
<li>1.我们用公式来明确表示与数据的关键属性有关的“管道”（方法）优化问题和分割算法的一些关键的设计选择；
<ul>
<li><font color=orange><strong>数据指纹（data fingerprint）：</strong></font>(表示数据集的关键属性)</li>
<li><font color=orange><strong>管道指纹（pipeline fingerprint）：</strong></font>（表示‘管道’关键的优化设计）</li>
</ul>
</li>
<li>2.我们通过将大多数的理论知识集成为一种启发性规则，来使得上面两者的关系更加的准确，这个规则可以从原有的<strong>data fingerprint</strong>中生成出<strong>pipeline fingerprint</strong>（即我可以通过数据的性质来推出它可能需要进行什么样子的训练），同时将硬件的局限条件考虑进去。</li>
</ul>
<p>        与专门为不同的训练任务调节不同的参数设置不同的算法不一样的是，nnUNet很容易就能执行系统性的规则，并依据看不见的数据集来生成相关的深度学习的方法，而且还不需要进一步的优化。
        接下来，我们会证实我们概念的先进性，看看各项挑战究竟是如何在我们的算法之下实现最优的效果的。这些强有力的结果更加证明了，对于那些想要做语义分割的使用者来说，nnUNet的巨大意义：nnUNet作为一款开源工具，可以被轻松的下载和使用，并且轻而易举就可以训练出SOTA级别的模型，甚至不需要专业的知识储备。我们进一步证实了最近的很多医学图像分割方法的缺点。我们尤其深入的的了解了2019年的KiTS挑战，同时证明了与选择一些相当顶尖的先进的Unet网络架构相比，针对不同任务而进行的不同的设计和参数的调整方法似乎更加重要。nnUNet让所有的研究人员对此更加充满热情，这样也就有了更多的实验结论可供参考，无形中为医学图像领域的方法论发展也起到了促进作用。</p>
<hr>
<h2 id="2结果">2.结果</h2>
<p>        nnUNet是一个不需要任何实验设计和参数调节就可以为你训练3D医学图像的深度学习语义分割框架。一些对于比较典型的数据集的分割效果在<strong>图一</strong>中展示。</p>
<ul>
<li>
<p>(图一)：<strong>nnUNet掌握了数据集的多样性和标签的属性：</strong> 以上所有的测试集都来自nnUNet对国际上各项挑战赛事的数据集的应用，左边是原数据的标签，右边是nnUNet模型的推理结果。所有这些可视化操作是在MITK上进行的。


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20200704222638819.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图一" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20200704222638819.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图一"/>
      </div>
  </div>
</figure>
</p>
</li>
<li>
<p>(图二)：<strong>人为调节的参数和本文提出的自动参数调节对比：</strong></p>
<ul>
<li>a) 最近普遍的深度学习的步骤：进行迭代式的训练和调参，训练之后评估结果，效果不好则进行调参，调参之后继续训练，最终进行侦测，反复如此；</li>
<li>b) nnUNet：
<ul>
<li>1.数据<font color=blue><strong>【无论是训练数据还是测试数据，都具有相对应的属性，即指纹】</strong></font>的属性会被总结成一种“数据指纹”；</li>
<li>2.一系列的<font color=orange><strong>启发式的规则</strong></font>会推理出适合这种指纹的“管道”（由蓝图参数（计划参数）推理得出）：</li>
<li>3.上一步推理出的参数，例如image resample、batch_size等，联合起来成为“管道指纹”：</li>
<li>4.2D、3D和3D_Cascaded三个网络分别训练，得出各自的模型（三个网络结构共享一个“管道指纹”，<strong>五折交叉</strong>验证）</li>
<li>5.选择出最优的模型进行推理（可以单个进行推理，也可以三个模型一起进行推理）


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20200704231733153.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20200704231733153.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="1nnunet可以自动适应任何新的数据">1.nnUNet可以自动适应任何新的数据</h3>
<p>        图二(a)展示了近来的多数医学图像分割是如何进行一个新的数据集的训练的。这个过程是“专家驱动”的，而且需要长时间的人为的试错的过程，显然，这样的训练方式对于手头上要处理的数据可能极少有先兆的参考。图二(b)中nnUNet却将这个自适应的过程系统化。
        因此，我们在此定义了一个类似于“图片大小”这样的标准数据集的属性——<strong>dataset fingerprint</strong>(数据指纹)，和一个<strong>pipeline fingerprint</strong>(管道指纹)[<strong>一个训练计划中各个配置的合体</strong>]。对于一个给定的“数据指纹”，nnUNet负责生成一个指定的“管道指纹”。</p>
<ul>
<li>在nnUNet中，这些“管道指纹”被分为<strong>蓝图参数</strong>、<strong>推理参数</strong>和<strong>经验参数</strong>。
<ul>
<li><strong>1.蓝图参数</strong>：
<ul>
<li>① 基础的网络架构选择：比如一个朴素的nnUNet网络；</li>
<li>② 易于选择的一些表现较好的常用超参数：比如损失函数、训练进度表、数据增强方式；</li>
</ul>
</li>
<li><strong>2.推理参数</strong>：
<ul>
<li>① 对一个新数据集进行适应性的编码，包括新的网络拓扑结构、patch_size、batch_size和图像预处理；</li>
<li>② ‘数据指纹’和‘管道指纹’两者之间的关系可以通过执行一系列<strong>启发式的规则</strong>来进行建立，而且遇到未知的数据集时也不需要昂贵的反复训练的代价；</li>
<li>③ 注意许多的设计选择都是相互依赖的：举个栗子
<ul>
<li>
<pre><code class="language-mermaid" data-lang="mermaid">  graph LR
  A[目标图像image_spacing] -- 影响 --&gt; B[image_size]
  B -- 确定 --&gt; C[训练时的patch]
  C -- 影响 --&gt; D[网络的拓扑结构]
  E[mini_batch] --为防止超显存而调整 --&gt; C
</code></pre><pre><code></code></pre></li>
<li>nnUNet卸去了人为解释这些依赖关系的压力</li>
</ul>
</li>
</ul>
</li>
<li><strong>3.经验参数</strong>：
<ul>
<li>① 经验参数只会在后面的推理时使用，这点从图b可以看出。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>        每进行一次训练，nnUNet都会为此创建三个不同的配置：</p>
<ul>
<li>2D、3D、3D_cascade
<ul>
<li>2D_Unet: 普通的2D_Unet</li>
<li>3D_Unet: 对一整张图片像素进行操作</li>
<li>3D_cascade: 级联网络：第一个网络对下采样图片进行操作，而第二个网络对前一个网络产生的结果在整个图片的像素上进行调整。</li>
</ul>
</li>
<li>在进行完交叉验证之后，nnUNet会经验性的选择表现最好的参数，可能是独个的推理结果，也可能是一起推理的结果。在结果可以评估的情况下，把对次优效果的抑制作为一项后处理操作。</li>
</ul>
<p>        nnUNet的输出是自适应的，对于未知的数据集，训练完整的模型同样能够做良好的预测。我们通过网络平台对nnUNet背后的方法论进行了深入的描述。我们最重要的设计原则（也就是我们对一个新的数据集训练提出的建议），都在后面附录的B中。而所有分割任务的“管道”的手稿，都放在附录的F中。</p>
<hr>
<h3 id="2nnunet极佳地掌握了目标标签的结构和数据图片的属性">2.nnUNet极佳地掌握了目标标签的结构和数据图片的属性</h3>
<p>        我们证明了nnUNet作为一个开箱即用的框架的价值：在19个数据集、共囊括49个分割任务上都有良好表现，这些任务包括器官的分割、器官的子结构分割、肿瘤分割、病灶分割等，而且实在多个模态下，例如MRI、CT、EM等。这些竞赛都是国际性的赛事，且被认为是临床实验算法的基准。很明显，举办赛事的人、机构、会社等都是不同的，所以挑战任务也具备独特性，他们的目的是评估在一个标准化的环境下，多个算法的表现。在所有的分割任务中，nnUNet仅仅使用竞赛公布的数据集。nnUNet对“十项全能数据”的方法论做了解释，其他的数据和任务也都进行了独立测试（简单应用并未进行调优）。从本质上说，nnUNet很好的处理了不同数据集的属性和不同标签的结构差异（所有生成的“管道”都被该领域的专家认为是合理甚至明智的）。结果在图一中有展示。</p>
<hr>
<h3 id="3nnunet在多个不同任务中的表现都要比一些刻意设计的管道要好">3.nnUNet在多个不同任务中的表现，都要比一些刻意设计的“管道”要好</h3>
<p>        大多数的国际挑战使用dice系数来作为效果的评估标准，dice的衡量标准是1为最优，但也有人用normalized_surface_dice（越大越好）和霍夫距离（越小越好），这两个都可以作为分割的边缘效果的衡量标准。<strong>图三</strong>展示了nnUNet在49项任务上的量化结果。</p>
<ul>
<li>(图三)：<strong>nnUNet的优越之处：</strong> 这是参加的49项竞赛的结果，红色点是nnUNet的表现结果，而蓝色点是其他队伍的结果。右下角的<strong>rank</strong>指的是<strong>nnUNet的此次排名/此次竞赛共提交的论文数</strong>。<strong>DC</strong>表示<strong>dice</strong>，<strong>OH</strong>是另一种衡量的分数（越高越好），<strong>OL</strong>也是一种衡量分数（越低越好）。该排行截止到2019.12。


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20200705093803405.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图三" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20200705093803405.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图三"/>
      </div>
  </div>
</figure>

        尽管其他参赛人员针对不同任务进行了反复的调优，但是效果仍然不如自然发生的nnUNet。总的来说，nnUNet总共有29项竞赛表现都成为了SOTA级别，而其他的也相当接近顶级水准。</li>
</ul>
<hr>
<h3 id="4管道配置对结果的影响要比网络结构变化对结果的影响大">4.“管道”配置对结果的影响要比网络结构变化对结果的影响大</h3>
<ul>
<li>(图四)：<strong>来自KiT2019的“管道指纹”：</strong>
<ul>
<li>a) 展示了各种各样的网络结构的变化，但是前15名全都用了skip_connection、3D卷积和类似于3D_Unet的网络结构，并没有新的变异的结构脱颖而出。同时，这些结构都无法保证足够优秀的结果，这说明网络结构的变化不足以支撑起好的模型效果；</li>
<li>b) 这是从所有参赛作品中挑选出来的非级联型且带有残差的、形如3D_Unet的网络的“管道指纹”，他们排名和“管道指纹”差别都相当大。看起来，这些模型的表现和一个独立的参数之间也没有明显的关联。<strong>CE</strong>: 交叉熵损失；<strong>Dice</strong>: soft dice loss; <strong>WBCE</strong>: weighted binary CE。</li>
</ul>
</li>
</ul>
<p>

<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/2020070510011363.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图四" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/2020070510011363.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图四"/>
      </div>
  </div>
</figure>

        为了强调任务设计的针对性和具体方法的配置的重要性要比整些花里胡哨的网络结构要重要的多，我们对KiT2019的参赛者的作品进行了分析，KiT2019属于MICCAI的竞赛，而MICCAI每年举办几乎一半的医学图像竞赛，该竞赛有100个参赛成员，是竞赛项目里人数最多的。</p>
<ul>
<li>我们对这项竞赛的数据分析透露了我们对于近期医学图像分割领域的一些看法：
<ul>
<li>1.排名前15的研究都是2016年的3D_UNet的产物；</li>
<li>2.使用相同的网络却会产生不同的结果；</li>
<li>3.更近一步的观察得出，前15名的研究普遍都没有用一些“高级”的网络设计（残差连接、稠密连接、注意力机制、膨胀卷积），由此可以看出最近很多的网络设计可能还没有最基础的网络效果好。</li>
</ul>
</li>
</ul>
<p>        图四则体现了对于使用同样的带有残差的3D_Unet网络架构来说，通过分析算法来进行超参数的调节是多么重要。当一个参赛者用这种方法赢得了竞赛，其他的基于这种规则的参赛作品可能覆盖整个的排行榜。图中选出了所有非级联型的带残差的Unet，并展示了他们的“管道指纹”，结果表明，每个队伍在进行“管道指纹”的设计都依据着这些“指纹”的相关性。这些看起来各有千秋的“管道指纹”，也恰恰证明了利用深度学习做医学图像分割时，超参数要进行高纬度优化的潜在复杂度。
        nnUNet通过对KiTS的数据集进行实验证明了，好的超参数对结果的利好大于网络结构的变异，而且仅仅用了一个3D_Unet。我们的结论通过被各种参赛者的采纳和引用，不断的被证实。</p>
<hr>
<h3 id="5不同的数据集需要不同管道配置">5.不同的数据集需要不同“管道”配置</h3>
<ul>
<li>(图五)：<strong>来自不同竞赛的不同数据集的“管道指纹”：</strong>
<ul>
<li>以下时nnUNet在19个数据集上的“管道指纹”（<font color=green><strong>附录A</strong></font>），可见“管道指纹”体现了数据集的关键性质（<font color=red>displayed on z-score normalized scale</font>）。数据集在他们的属性上体现出了巨大的差异，所以一方面需要很努力的适应不同的数据集，另一方面需要很大量的数据的支撑来得到其方法结论。右下角时三个模态。


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20200705114842912.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图五" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20200705114842912.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图五"/>
      </div>
  </div>
</figure>
</li>
</ul>
</li>
</ul>
<p>        我们抽取了19个实验数据集和相对应的“管道指纹”，并展示在图五中。这记录了在医学图像领域，数据集具有超乎寻常的复杂性。同时也暴露了为什么其他算法不如nnUNet的原因：
        我们进行一次“管道指纹”的设计，要么就是人为的设置，要么就是利用各个“数据指纹”之间的隐藏关系进行设置。结果可能就是，对于一个数据集来说优化的效果很好，而对于其他数据集可能没有什么作用，对于一个新的数据集，就需要人为的不断的进行再建计划和再次优化。比如图片的尺寸会影响patch_size，patch_size会反过来影像网络的拓扑结构（下采样的次数和卷积核的尺寸等），而网络的拓扑结构将再次影响其他的超参数。</p>
<hr>
<h3 id="6多任务提升了决策方案的鲁棒性">6.多任务提升了决策（方案）的鲁棒性</h3>
<ul>
<li>(图六)：<strong>对多个任务的决策进行评估”：</strong>
<ul>
<li>以下时nnUNet在19个数据集上的“管道指纹”（<font color=green><strong>附录A</strong></font>），可见“管道指纹”体现了数据集的关键性质（<font color=red>displayed on z-score normalized scale</font>）。数据集在他们的属性上体现出了巨大的差异，所以一方面需要很努力的适应不同的数据集，另一方面需要很大量的数据的支撑来得到其方法结论。右下角时三个模态。</li>
<li><font color=purple><strong>上段话我省略了作者对该图的解释，所以对该图增加解释，确实只看图难以理解</strong></font>
<ul>
<li>1.参考作者对该问题的回答：


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20200802113519465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="1" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20200802113519465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="1"/>
      </div>
  </div>
</figure>
</li>
<li>2.下面是我对该图的理解：
<ul>
<li>① 只单看左上角的a), 每种颜色对应的是使用了什么样的优化方式，这个BrainTumor的训练集会被作者在进行1000次的5折交叉验证时生成1000个虚拟验证集（这里作者的回答应该是说错了），然后我们在这个训练集上进行9种训练，分别对应下面9种颜色，每种都基于nnUNet本身的“蓝图参数”做一些小的更改（控制变量法），然后让训练的九个模型分别在1000个不同的验证集上进行推理和测试，并将他们进行分数（dice）的排名，mean（就是那个竖线）是1000个排名的平均排名。你所看到的每一个颜色的柱状都是一个模型在不同验证集上的结果。其他几个表一样是这个道理，除了最后一个。</li>
<li>② 最后一个是对前面10个做的一次整体性的评估，也就是不管哪个任务的排名，全部总结在这个表里，最终得出的结果是，nnUNet不适用这些方法和技巧，依然可以排名第一保持极高的水平。


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/2020070512281911.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图六" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/2020070512281911.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70#pic_center" alt="图六"/>
      </div>
  </div>
</figure>

        nnUNet是能够使研发人员不必为不同的数据集人为的调整整个“管道”。为了证明这一点，同时为了支持我们在nnUNet中做的一些核心的设计，我们系统的测试了，nnUNet的蓝图参数结合下面这些用的比较多的配置在十项全能的十个数据集上表现。（图六包含了两个备选损失函数【①CE和②TopK10】、③编码阶段的残差连接、④每层的下采样从两次卷积变为三次卷积【使得网络更深】、两种优化器的修改【⑤一种使减小优化器的冲量，⑥一种是备用的Adam优化器】、⑦用batch_normal来替代instance_normal、以及⑧完全不用任何优化【最最普通的unet网络】）。根据挑战者工具的建议，通过bootstrapping来估计排名的稳定性。
        各数据集排名的波动性表明，超参数影像图像分割表现的程度取决于数据集。结果清晰的显示，如果是对 一个数目不足的数据集做方法论的评估一定要十分的小心。<font color=red>虽然9个变量中的5个在至少一个数据集中达到了排名1，但它们都没有在10个任务中显示出一致的改进。原始的nnU-Net配置在聚合所有数据集的结果时表现出最佳的泛化效果。</font>
        在最近的研究中，一次评估很少用在两个以上的不同的数据集上，甚至两个不同的数据集还可能使同一模态，比如全是CT。就像我们展示的一样，这样的评估方法对于方法论的总结来说使不合理的。我们将缺乏足够广泛的评估与将现有管道调整到单个数据集所需的手动调优工作联系起来。nnU-Net以两种方式缓解了这一缺点：
<ul>
<li>作为一个可扩展的框架，支持跨多个任务有效地评估新概念；</li>
<li>作为一个即插即用的、标准化和最先进的基准，用于和一些评估做比较。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3讨论">3.讨论</h2>
<p>        第一段重复了nnUNet无需人为介入且表现优异的先进性。
        第二段重复改变网络结构对于提升效果的意义不大，而且很多基于一种数据集训练的模型对于新的数据集没有参考价值。
        第三段重复nnUNet的实际意义：一、对任意数据集可进行相应的调整；二、其对任意数据集的实验结果可以作为一种基准，来评判其他分割算法的好坏。
        第四段将nnUNet和AutoML等作比较，阐述了它的优点，自动创建决策的时候会消耗更少的资源，而且具有很强的适应性和稳定性。
        第五段提出nnUNet的局限性：1.举例说明了对于显微镜光学图像的预处理很可能会提升nnUNet的效果；2.nnUNet在两个方面仍然不是最优：<font color=red>① 对于一些潜在的cases，nnUNet自身的启发机制很有可能会随之扩展</font>；② 对于一些极其专业的领域cases，nnUNet只能作为一个起点。
        第六段继续重复自己的牛逼之处。</p>
<hr>
<p>本文依赖：</p>
<ul>
<li><a href="https://github.com/MIC-DKFZ/nnUNet" target="_blank" rel="noopener">框架源码<i class="fas fa-external-link-alt"></i></a></li>
<li><a href="https://arxiv.org/pdf/1904.08128.pdf" target="_blank" rel="noopener">论文源址<i class="fas fa-external-link-alt"></i></a></li>
<li><font color=blue>个人对于本文章的解释：
<ul>
<li>1.本文的翻译仅建立在个人的知识储备和截止现在为止对nnUNet的使用理解，非常欢迎各位的指正，本人分享理解的目的与论文作者的开源目的如出一辙，希望本框架乃至医学图像分割领域有更广阔的发展空间；</li>
<li>2.红字标识是亟待继续理解的部分，很多部分仍然理解的较为浅显，因为尚未进行框架的完全开发使用和对随文目录的理解，本论文附录内容我将持续解读并发表；</li>
<li>3.本文将会持续进行修改和更新，框架的相关使用方法和错误处理将会毫无保留的在之后的更新中推出，但碍于研发工作的私密性，训练数据和表现将不便透露，希望老铁们理解。</li>
<li>4.nnUNet牛逼！</li>
</ul>
</li>
</ul>



      </main>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://twitter.com/grpcio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Google Groups" aria-label="Google Groups">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://groups.google.com/g/grpc-io">
      <i class="fab fa-google"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Gitter" aria-label="Gitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://gitter.im/grpc/grpc">
      <i class="fab fa-gitter"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://github.com/grpc">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 gRPC Authors
          
        </small>
        
        
      </div>
    </div>
    <div class="row text-center text-white small">
      <div class="col-12 text-center py-2 order-sm-2">
        <a href="https://www.linuxfoundation.org/terms" target="_blank" rel="noopener">Terms</a> |
        <a href="https://www.linuxfoundation.org/privacy" target="_blank" rel="noopener">Privacy</a> |
        <a href="https://www.linuxfoundation.org/trademark-usage" target="_blank" rel="noopener">Trademarks</a> |
        <a href="https://github.com/grpc/grpc.io/blob/main/LICENSE" target="_blank" rel="noopener">License</a> |
        <a href="/about/">About</a>
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>











<script src="/js/main.min.d862e8117d34e02dff1772aeeb47caf7ed851d0f8a3a5345f7a32ccfb2f6b87f.js" integrity="sha256-2GLoEX004C3/F3Ku60fK9&#43;2FHQ&#43;KOlNF96Msz7L2uH8=" crossorigin="anonymous"></script>




  </body>
</html>