<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.82.1" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">
<title>HRNet用于视觉识别的深度高分辨率表示学习 | gRPC</title>
<meta property="og:title" content="HRNet用于视觉识别的深度高分辨率表示学习" />
<meta property="og:description" content="HRNet用于视觉识别的深度高分辨率表示学习       Contents   低分辨率表示学习 高分辨率表示恢复 高分辨率表示保持 多尺度混合 我们的方法   3.1 并行多分辨率卷积 3.2 重复的多分辨率融合        讲在前面  一.学习该算法的目的：在u2net上进行针道的尝试之后，发现分割的算法无法解决针道问题，能呈现一定的分割效果，但因为针在体内的位置不确定，性状、像素值也并不像之前设想的那么具有可识别度，故希望利用从feature map上回归点的特征点的方法解决这个棘手的问题； 二.我设计了几种字体颜色用于更加醒目地表现关键的思想和主题：  红色表示尚未理解透彻的一些概念 蓝色表示对原来的理解做的一些修改或补充 绿色表示此处需要参考的论文其他部分 橙色表示本文的重要关键字 紫色表示后续更新的内容 我会用删除线将自己曾经不到位的理解进行删除   三.CVPR2019 的 HRNet 在我之前的工作中具有十分重要的作用，在长期的图像分割任务中，并没有太多时间思考这个模型及类似点回归模型在医学图像中的应用。从这篇博客开始，我将会持续探讨，此类点回归模型在医学图像中介入体识别中的作用。  一、摘要 深度卷积网络在许多机器视觉领域已经取得了SOTA级别的效果，比如图像分类、目标检测、语义分割、人体姿态识别等等。原因就是与手工制作的特征相比，深度学习具有更强大的提取特征的能力。 最近的大多数分类网络，比如AlexNet、VGGNet、GoogleNet、ResNet等等，都遵从着LeNet-5的设计原则，这个原则就如 （图一） 所描述的：
 (a) 从高分辨率到低分辨率逐渐减小特征图的尺寸，最终从低分辨率的特征图中得到分类的结果； 图一：      高分辨率表达主要用于一些对位置敏感的任务，比如语义分割、人体姿态识别和目标检测。先前的SOTA方法主要采用的是高分辨率恢复的方法，将低分辨率图中的分类结果提升为高分辨率，如图一的(b)中描述的，比如Hourglass、SegNet、DeconvNet、U-Net、SimpleBaseline和编解码模式。另外，膨胀卷积用于移除一些下采样层，从而产生一些中级像素的特征图。 我们提出了一个新的网络结构，名为 High-ResolutionNet (HRNet)，能够在整个过程中保持高分辨率的特征图。我们从一个高分辨率的卷积流程开始，一个个地把由高到低的卷积流加入进去，然后平行连接多个流。最终的网络包含了几个（论文中是4个）图二所表示出的stage，第n个stage包含了对应与n像素级的n个流。我们通过在并行流之间交换信息来进行重复的多分辨率融合。
 图二：      HRNet的高分辨率表达不仅仅在语义上具有很好的性能，在空间上也具备一定的精准度。这主要来自两个方面：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Joevaen.github.io/showcase/paper/paper7/" /><meta property="article:section" content="showcase" />

<meta property="og:site_name" content="gRPC" />

<meta itemprop="name" content="HRNet用于视觉识别的深度高分辨率表示学习">
<meta itemprop="description" content="HRNet用于视觉识别的深度高分辨率表示学习       Contents   低分辨率表示学习 高分辨率表示恢复 高分辨率表示保持 多尺度混合 我们的方法   3.1 并行多分辨率卷积 3.2 重复的多分辨率融合        讲在前面  一.学习该算法的目的：在u2net上进行针道的尝试之后，发现分割的算法无法解决针道问题，能呈现一定的分割效果，但因为针在体内的位置不确定，性状、像素值也并不像之前设想的那么具有可识别度，故希望利用从feature map上回归点的特征点的方法解决这个棘手的问题； 二.我设计了几种字体颜色用于更加醒目地表现关键的思想和主题：  红色表示尚未理解透彻的一些概念 蓝色表示对原来的理解做的一些修改或补充 绿色表示此处需要参考的论文其他部分 橙色表示本文的重要关键字 紫色表示后续更新的内容 我会用删除线将自己曾经不到位的理解进行删除   三.CVPR2019 的 HRNet 在我之前的工作中具有十分重要的作用，在长期的图像分割任务中，并没有太多时间思考这个模型及类似点回归模型在医学图像中的应用。从这篇博客开始，我将会持续探讨，此类点回归模型在医学图像中介入体识别中的作用。  一、摘要 深度卷积网络在许多机器视觉领域已经取得了SOTA级别的效果，比如图像分类、目标检测、语义分割、人体姿态识别等等。原因就是与手工制作的特征相比，深度学习具有更强大的提取特征的能力。 最近的大多数分类网络，比如AlexNet、VGGNet、GoogleNet、ResNet等等，都遵从着LeNet-5的设计原则，这个原则就如 （图一） 所描述的：
 (a) 从高分辨率到低分辨率逐渐减小特征图的尺寸，最终从低分辨率的特征图中得到分类的结果； 图一：      高分辨率表达主要用于一些对位置敏感的任务，比如语义分割、人体姿态识别和目标检测。先前的SOTA方法主要采用的是高分辨率恢复的方法，将低分辨率图中的分类结果提升为高分辨率，如图一的(b)中描述的，比如Hourglass、SegNet、DeconvNet、U-Net、SimpleBaseline和编解码模式。另外，膨胀卷积用于移除一些下采样层，从而产生一些中级像素的特征图。 我们提出了一个新的网络结构，名为 High-ResolutionNet (HRNet)，能够在整个过程中保持高分辨率的特征图。我们从一个高分辨率的卷积流程开始，一个个地把由高到低的卷积流加入进去，然后平行连接多个流。最终的网络包含了几个（论文中是4个）图二所表示出的stage，第n个stage包含了对应与n像素级的n个流。我们通过在并行流之间交换信息来进行重复的多分辨率融合。
 图二：      HRNet的高分辨率表达不仅仅在语义上具有很好的性能，在空间上也具备一定的精准度。这主要来自两个方面：">

<meta itemprop="wordCount" content="148">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="HRNet用于视觉识别的深度高分辨率表示学习"/>
<meta name="twitter:description" content="HRNet用于视觉识别的深度高分辨率表示学习       Contents   低分辨率表示学习 高分辨率表示恢复 高分辨率表示保持 多尺度混合 我们的方法   3.1 并行多分辨率卷积 3.2 重复的多分辨率融合        讲在前面  一.学习该算法的目的：在u2net上进行针道的尝试之后，发现分割的算法无法解决针道问题，能呈现一定的分割效果，但因为针在体内的位置不确定，性状、像素值也并不像之前设想的那么具有可识别度，故希望利用从feature map上回归点的特征点的方法解决这个棘手的问题； 二.我设计了几种字体颜色用于更加醒目地表现关键的思想和主题：  红色表示尚未理解透彻的一些概念 蓝色表示对原来的理解做的一些修改或补充 绿色表示此处需要参考的论文其他部分 橙色表示本文的重要关键字 紫色表示后续更新的内容 我会用删除线将自己曾经不到位的理解进行删除   三.CVPR2019 的 HRNet 在我之前的工作中具有十分重要的作用，在长期的图像分割任务中，并没有太多时间思考这个模型及类似点回归模型在医学图像中的应用。从这篇博客开始，我将会持续探讨，此类点回归模型在医学图像中介入体识别中的作用。  一、摘要 深度卷积网络在许多机器视觉领域已经取得了SOTA级别的效果，比如图像分类、目标检测、语义分割、人体姿态识别等等。原因就是与手工制作的特征相比，深度学习具有更强大的提取特征的能力。 最近的大多数分类网络，比如AlexNet、VGGNet、GoogleNet、ResNet等等，都遵从着LeNet-5的设计原则，这个原则就如 （图一） 所描述的：
 (a) 从高分辨率到低分辨率逐渐减小特征图的尺寸，最终从低分辨率的特征图中得到分类的结果； 图一：      高分辨率表达主要用于一些对位置敏感的任务，比如语义分割、人体姿态识别和目标检测。先前的SOTA方法主要采用的是高分辨率恢复的方法，将低分辨率图中的分类结果提升为高分辨率，如图一的(b)中描述的，比如Hourglass、SegNet、DeconvNet、U-Net、SimpleBaseline和编解码模式。另外，膨胀卷积用于移除一些下采样层，从而产生一些中级像素的特征图。 我们提出了一个新的网络结构，名为 High-ResolutionNet (HRNet)，能够在整个过程中保持高分辨率的特征图。我们从一个高分辨率的卷积流程开始，一个个地把由高到低的卷积流加入进去，然后平行连接多个流。最终的网络包含了几个（论文中是4个）图二所表示出的stage，第n个stage包含了对应与n像素级的n个流。我们通过在并行流之间交换信息来进行重复的多分辨率融合。
 图二：      HRNet的高分辨率表达不仅仅在语义上具有很好的性能，在空间上也具备一定的精准度。这主要来自两个方面："/>



<link rel="preload" href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" as="style">
<link href="/css/style.min.e3cf82e558200e98b09736f835e12b188dca1353aad6649d3429c72923031431.css" rel="stylesheet" integrity="">

<script
  src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
  crossorigin="anonymous"></script>




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-163836834-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-163836834-2');
  gtag('config', 'UA-60127042-1');
</script>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="canonical" href="https://Joevaen.github.io/showcase/paper/paper7/">

<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@grpcio">
<meta name="twitter:creator" content="@grpcio">
<meta name="twitter:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta name="twitter:image:alt" content="gRPC color logo">

<meta property="og:url" content="https://Joevaen.github.io/showcase/paper/paper7/">
<meta property="og:title" content="HRNet用于视觉识别的深度高分辨率表示学习">
<meta property="og:description" content="A high-performance, open source universal RPC framework">
<meta property="og:type" content="article">
<meta property="og:site_name" content="gRPC">
<meta property="og:image" content="https://Joevaen.github.io/img/logos/grpc-icon-color.png">
<meta property="og:image:type" content="image/png">
<meta property="og:image:alt" content="gRPC color logo">
<meta property="og:locale" content="en_US">
<link rel="shortcut icon" href="/favicon.ico">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" href="/favicons/android-chrome-192x192.png" sizes="192x192">
<link rel="icon" type="image/png" href="/favicons/android-chrome-512x512.png" sizes="512x512">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png" sizes="16x16">
<link rel="manifest" href="/favicons/site.webmanifest">

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  td-navbar-cover flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/">
		<span class="navbar-logo"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="672pt" height="436pt" viewBox="0 0 672 436"><g transform="translate(0.000000,436.000000) scale(0.100000,-0.100000)" fill="#000" stroke="none"><path d="M3066 3659c-376-41-707-264-894-602-35-63-50-101-45-113 4-12 3-15-5-10s-10 1-5-10c5-13 9-11 23 15 9 18 19 29 22 25 4-3 4 3 1 15-4 14-1 21 9 21 8 0 27 23 42 51 119 218 360 416 607 499 96 33 250 60 337 60 551 0 1009-390 1097-933 48-296-20-597-193-847-24-36-35-56-23-46 35 32 24 2-23-61-49-64-119-128-220-198-36-26-66-49-66-51 0-10 123 74 187 128 63 53 173 174 182 201 2 7 9 19 16 27s36 58 63 110c198 373 185 786-35 1159-25 42-60 95-79 118-42 51-45 68-4 22 68-76 190-285 190-326 0-7 5-12 11-10 6 1 14-4 17-12 2-8 0-11-5-8-6 4-8-4-5-19 3-18 7-22 12-13 6 8 12-2 19-29 6-23 15-59 21-80 31-121 29-332-5-516-9-44-13-81-11-83s16 47 31 109c26 106 27 122 22 268-4 119-12 179-32 260-56 225-191 440-375 598-70 59-261 182-284 182-6 0 25-21 69-47 44-25 105-65 135-89 62-48 149-134 135-134-6 0-32 23-60 51-67 67-224 171-325 215-78 34-249 83-294 85-21 0-21 1-1 9 15 7 8 9-30 10-27 0-72 2-1e2 4-27 2-86 0-129-5z"/><path d="M36e2 3580c8-5 20-10 25-10 6 0 3 5-5 10s-19 10-25 10c-5 0-3-5 5-10z"/><path d="M3519 3276c-69-18-136-62-211-138-191-194-325-488-288-630 12-45 45-88 67-88 18 0 16 19-2 27-24 9-45 57-45 103 0 121 141 397 271 531 111 115 241 189 303 173 1e2-25 76-265-64-626l-40-105-58-13c-79-17-211-65-309-110-197-92-359-233-429-375-35-71-39-87-39-154 0-65 4-83 28-123 61-104 188-133 299-70 70 41 176 152 251 265 67 102 188 333 243 464 37 89 39 92 79 102 43 11 84 36 74 45-3 3-24-2-47-11s-45-14-48-11 15 63 41 133c126 349 145 579 49 615-33 13-68 12-125-4zm-29-794c0-4-47-102-104-218-197-398-362-594-5e2-594-68 0-124 44-151 119-85 243 203 545 645 675 93 27 110 30 110 18z"/><path d="M2081 2794c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4291 2784c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2071 2754c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M4302 2740c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2061 2714c0-11 3-14 6-6 3 7 2 16-1 19-3 4-6-2-5-13z"/><path d="M2052 2660c0-14 2-19 5-12 2 6 2 18 0 25-3 6-5 1-5-13z"/><path d="M2043 2585c0-22 2-30 4-17 2 12 2 30 0 40-3 9-5-1-4-23z"/><path d="M4252 2018c-15-37-19-55-11-63 7-7 8-4 3 9-6 15-4 17 6 11 9-5 11-4 6 3-4 7-3 12 3 12s7 7 4 17c-5 12-3 14 8 8 8-4 11-5 7 0-4 4-4 13 2 19 5 7 5 17 1 24s-15-8-29-40z"/><path d="M4205 1919c-3-4 2-6 10-5 21 3 28 13 10 13-9 0-18-4-20-8z"/><path d="M4024 1668l-19-23 23 19c12 11 22 21 22 23 0 8-8 2-26-19z"/><path d="M4015 1628l-40-43 43 40c39 36 47 45 39 45-2 0-21-19-42-42z"/><path d="M3969 1613c-13-16-12-17 4-4s21 21 13 21c-2 0-10-8-17-17z"/><path d="M3919 1543c-13-16-12-17 4-4 9 7 17 15 17 17 0 8-8 3-21-13z"/><path d="M3705 1428c-11-6-23-15-26-20-3-6-9-9-13-9-19 3-27 0-22-7 7-12-61-35-75-26-8 5-9 3-4-6 6-10 3-12-13-8-14 4-20 2-15-5 4-6-2-8-15-5-12 4-20 2-17-2 3-5 0-10-7-13-7-2-2-2 12 0 39 7 179 63 194 78 7 8 16 12 18 10 3-3 5 2 5 10s-1 15-1 15c-1 0-10-6-21-12z"/><path d="M3448 1313c7-3 16-2 19 1 4 3-2 6-13 5-11 0-14-3-6-6z"/><path d="M3355 13e2c-27-7-27-8-5-8 14 0 39 4 55 8 27 7 27 8 5 8-14 0-38-4-55-8z"/><path d="M3263 1283c9-2 25-2 35 0 9 3 1 5-18 5s-27-2-17-5z"/><path d="M1810 981c0-75-4-103-15-115-17-17-20-66-4-66 6 0 24 13 40 29 29 29 29 31 29 140v111h-25c-25 0-25 0-25-99z"/><path d="M2211 1062c-51-25-73-61-74-120-1-70 33-116 101-137 36-10 28 31-13 69-26 25-35 42-35 66s9 41 35 66c19 18 35 42 35 53 0 25-5 26-49 3z"/><path d="M2290 1060c0-12 16-36 35-54 26-25 35-42 35-66s-9-41-35-66c-41-38-49-79-12-69 66 20 99 65 99 135s-33 115-99 135c-19 5-23 2-23-15z"/><path d="M2692 1053l3-28h1e2 1e2l3 28 3 27h-106-106l3-27z"/><path d="M3150 1068c0-7 23-70 52-140 48-119 53-128 78-128s30 9 78 130c29 72 52 135 52 141s-12 9-27 7c-23-2-31-13-51-63-13-33-31-76-39-95l-14-35-25 59c-13 33-24 63-24 67s15 9 33 11c27 2 32 7 32 28 0 24-3 25-72 28-55 2-73 0-73-10z"/><path d="M3724 1003c-43-93-48-128-19-128 15 0 28 17 50 63l30 62 25-62c14-35 27-66 28-70 2-5-35-8-82-8-76 0-86-2-1e2-22-8-12-13-25-10-30 7-11 261-10 268 1 3 5-19 67-49 137-48 114-57 129-80 132s-29-4-61-75z"/><path d="M4187 1073c-4-3-7-14-7-24 0-25 28-31 125-27 79 3 80 3 83 31l3 27h-99c-54 0-102-3-105-7z"/><path d="M4670 1068c0-7 43-71 96-141 77-105 99-128 117-125 21 3 22 7 25 141l3 138-28-3c-28-3-28-4-33-86l-5-83-62 85c-50 69-68 86-88 86-14 0-25-6-25-12z"/><path d="M2694 955c-14-37 2-45 89-45 88 0 114 11 101 44-9 23-182 24-190 1z"/><path d="M4184 955c-3-8-3-22 0-30 5-13 22-15 98-13 92 3 93 3 93 28s-1 25-93 28c-76 2-93 0-98-13z"/><path d="M1675 890c-10-16 4-48 32-70 32-25 40-25 48 0 13 40-60 103-80 70z"/><path d="M4670 850c0-47 2-50 25-50s25 3 25 50-2 50-25 50-25-3-25-50z"/><path d="M2694 845c-3-8-3-22 0-30 5-13 23-15 103-13 97 3 98 3 98 28s-1 25-98 28c-80 2-98 0-103-13z"/><path d="M4184 845c-15-38 1-45 106-45h101l-3 28-3 27-98 3c-80 2-98 0-103-13z"/></g></svg></span><span class="text-uppercase font-weight-bold">gRPC</span>
	</a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/about/" ><span>nnUNet</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/" ><span>MMDetection</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/showcase/" ><span>论文</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/" ><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/" ><span>C&#43;&#43;</span></a>
			</li>
			
			
			
		</ul>
	</div>
	<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">

</div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      
      <main role="main" class="td-main">
        












<section id="td-cover-block-0" class="row td-cover-block td-cover-block--height-sm js-td-cover td-overlay td-overlay--dark -bg-primary">
  <div class="container td-overlay__inner">
    <div class="row">
      <div class="col-12">
        <div class="text-center">
          
          
          <div class="pt-3 lead">
            
                <div class="text-left">
  <h1 class="display-1 mb-5">HRNet用于视觉识别的深度高分辨率表示学习</h1></div>

            
          </div>
        </div>
      </div>
    </div>
  </div>
  
</section>

<div class="container l-container--padded">
<div class="row">




  
    <div class="d-lg-none col-12">
      <div class="td-toc td-toc--inline">
  
      
        <a id="td-content__toc-link" class="collapsed" href="#td-content__toc" data-toggle="collapse" aria-controls="td-page-toc" aria-expanded="false" aria-label="Toggle toc navigation">
          <span class="lead">Contents<i class="fas fa-chevron-right ml-2"></i></span>
        </a>
        <div id="td-content__toc" class="collapse">
          <nav id="TableOfContents">
  <ul>
    <li><a href="#低分辨率表示学习">低分辨率表示学习</a></li>
    <li><a href="#高分辨率表示恢复">高分辨率表示恢复</a></li>
    <li><a href="#高分辨率表示保持">高分辨率表示保持</a></li>
    <li><a href="#多尺度混合">多尺度混合</a></li>
    <li><a href="#我们的方法">我们的方法</a></li>
  </ul>

  <ul>
    <li><a href="#31-并行多分辨率卷积">3.1 并行多分辨率卷积</a></li>
    <li><a href="#32--重复的多分辨率融合">3.2  重复的多分辨率融合</a></li>
  </ul>
</nav>
        </div>
        <button id="td-content__toc-link-expanded" href="#td-content__toc" class="btn btn-small ml-1 my-2 py-0 px-3" data-toggle="collapse" aria-controls="td-docs-toc" aria-expanded="true" aria-label="Toggle toc navigation">
        </button>
      
    </div>
  </div>

</div>
<div class="row">
<div class="col-12 col-lg-8">
<h1 id="讲在前面">讲在前面</h1>
<ul>
<li>一.学习该算法的目的：在u2net上进行针道的尝试之后，发现分割的算法无法解决针道问题，能呈现一定的分割效果，但因为针在体内的位置不确定，性状、像素值也并不像之前设想的那么具有可识别度，故希望利用从feature map上回归点的特征点的方法解决这个棘手的问题；</li>
<li>二.我设计了几种字体颜色用于更加醒目地表现关键的思想和主题：
<ul>
<li><strong><font color=red>红色表示尚未理解透彻的一些概念</font></strong></li>
<li><strong><font color=blue>蓝色表示对原来的理解做的一些修改或补充</font></strong></li>
<li><strong><font color=green>绿色表示此处需要参考的论文其他部分</font></strong></li>
<li><strong><font color=orange>橙色表示本文的重要关键字</font></strong></li>
<li><strong><font color=purple>紫色表示后续更新的内容</font></strong></li>
<li><strong><del>我会用删除线将自己曾经不到位的理解进行删除</del></strong></li>
</ul>
</li>
<li>三.CVPR2019 的 HRNet 在我之前的工作中具有十分重要的作用，在长期的图像分割任务中，并没有太多时间思考这个模型及类似点回归模型在医学图像中的应用。从这篇博客开始，我将会持续探讨，此类点回归模型在医学图像中介入体识别中的作用。</li>
</ul>
<h1 id="一摘要">一、摘要</h1>
<p>深度卷积网络在许多机器视觉领域已经取得了SOTA级别的效果，比如图像分类、目标检测、语义分割、人体姿态识别等等。原因就是与手工制作的特征相比，深度学习具有更强大的提取特征的能力。
最近的大多数分类网络，比如AlexNet、VGGNet、GoogleNet、ResNet等等，都遵从着LeNet-5的设计原则，这个原则就如 （<strong>图一</strong>） 所描述的：</p>
<ul>
<li>(a) 从高分辨率到低分辨率逐渐减小特征图的尺寸，最终从低分辨率的特征图中得到分类的结果；</li>
<li><strong>图一：</strong>


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20210412094000326.png" alt="在这里插入图片描述" id="20210412094000326" data-toggle="modal" data-target="#modal-20210412094000326"/>

  <div class="modal" id="modal-20210412094000326">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20210412094000326.png" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>
</li>
</ul>
<p>高分辨率表达主要用于一些对位置敏感的任务，比如语义分割、人体姿态识别和目标检测。先前的SOTA方法主要采用的是高分辨率恢复的方法，将低分辨率图中的分类结果提升为高分辨率，如<strong>图一</strong>的(b)中描述的，比如Hourglass、SegNet、DeconvNet、U-Net、SimpleBaseline和编解码模式。另外，膨胀卷积用于移除一些下采样层，从而产生一些中级像素的特征图。
我们提出了一个新的网络结构，名为 High-ResolutionNet (<strong>HRNet</strong>)，能够在整个过程中保持高分辨率的特征图。我们从一个高分辨率的卷积流程开始，一个个地把由高到低的卷积流加入进去，然后平行连接多个流。最终的网络包含了几个（论文中是4个）<strong>图二</strong>所表示出的stage，第n个stage包含了对应与n像素级的n个流。我们通过在并行流之间交换信息来进行重复的多分辨率融合。</p>
<ul>
<li><strong>图二：</strong>


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20210412113232752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20210412113232752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>
</li>
</ul>
<p>HRNet的高分辨率表达不仅仅在语义上具有很好的性能，在空间上也具备一定的精准度。这主要来自两个方面：</p>
<ul>
<li>(i) 我们的方法不仅包含了从低像素恢复的高分辨率信息，也包含了原始的高分辨率信息，因此，这样学习到的信息会使得空间上的判别更精确；</li>
<li>(ii) 当前大多数的融合方案，聚合了通过对低分辨率信息进行上采样获得的高分辨率和低分辨率，从而实现低级和高级表示。 相反，我们在低分辨率表示的帮助下重复多分辨率融合以增强高分辨率表示，反之亦然。 结果，我们的方法在语义上是强大的。</li>
</ul>
<p>我们提出了两个HRNet版本，<strong>HRNetV1</strong>只输出从高分辨率卷积流中计算出的高分辨率表达，然后将其应用到姿态评估的heatmap框架。我们以经验的方式展示了COCO关键点检测数据集上优越的姿态估计性能。
<strong>HRNetV2</strong>则将所有高到低像素流的表达进行了合并。通过从组合的高分辨率表示估计分割图，将其应用于语义分割。所提出的方法在PASCAL-Context、Cityscapes和LIP实现了SOTA结果，同时它的模型尺寸与之前的SOTA的模型尺寸差不多同时具有更低的计算复杂度。我们发现V2和V1在COCO数据集上的表现基本相同，同时发现了V2在语义分割上的先进性。
此外，我们从<strong>HRNetV2</strong>的高分辨率输出构建了一个多层表示，称为<strong>HRNetV2p</strong>，并将其应用于最新的检测框架，包括Faster R-CNN，Cascade R-CNN，FCOS和CenterNet，以及最新的联合检测和实例分割框架，包括Mask R-CNN，Cascade Mask R-CNN和Hybrid Task Cascade。结果表明，我们的方法改进了检测性能，特别是对小物体的动态改进。</p>
<h1 id="二相关工作">二、相关工作</h1>
<p>我们从三个方面回顾了主要用于人体姿势估计，语义分割和对象检测的紧密相关的表示学习技术：低分辨率表示学习，高分辨率表示恢复和高分辨率表示保持。 此外，我们还提到了与多尺度融合相关的一些作品。</p>
<h2 id="低分辨率表示学习">低分辨率表示学习</h2>
<p>全卷积网络方法，通过删除分类网络中的全连接层来计算低分辨率表示，并估计其粗略分割图。 通过组合从中间低级别中分辨率表示估计的精细分割得分图或对过程进行迭代，可以改善估计的分割图。 类似的技术也已应用于边缘检测，例如整体边缘检测。
<strong><font color=red>The  fully  convolutional  network  is  extended,  by  re-placing  a  few  (typically  two)  strided  convolutions  and the  associated  convolutions  with  dilated  convolutions,  tothe  dilation  version,  leading  to  medium-resolution  repre-sentations</font></strong>。通过特征金字塔进一步扩展为多尺度上下文表示，用于在多个尺度上分割对象。</p>
<h2 id="高分辨率表示恢复">高分辨率表示恢复</h2>
<p>上采样的操作主要用于逐渐把低分辨率的表示恢复成高分辨率信息，可以看做是下采样的对称版本（比如VGGNet），通过一些跳级链接来对池化层进行一些变换（比如SegNet和DeconvNet），或者复制特征图（比如UNet、Hourglass、编解码）。一种UNet的扩展——全像素残差网络，介绍了一种额外的全像素流，这种流通过在全图上拾取信息，代替了之前的跳级链接，下采样和上采样的每一个子网络单元都从全像素流上收到和发出信息。
这种非对称的上采样过程也被广泛研究，RefineNet改善了上采样信息表达和相同像素级别上、由下采样复制而来的信息表达。其他的工作包括：</p>
<ul>
<li>1.在backbone中使用可能带有扩张卷积的轻量级上采样方法；</li>
<li>2.轻量级下采样和重量级上采样组成的复合网络；</li>
<li>3.利用更多或者更复杂的卷积单元改进跳级链接，通过低像素的跳级链接向高像素的跳级链接发送信息或者交换这两者的信息；</li>
<li>4.研究上采样过程的细节；</li>
<li>5.组合多尺度金字塔的表达；</li>
<li>6.用密集链接来堆叠多个DeconvNets/UNet/Hourglass。</li>
</ul>
<h2 id="高分辨率表示保持">高分辨率表示保持</h2>
<p>我们的工作与之前的一些注重高像素表达的工作密切相关。比如 convolutional neural fabrics、interlinked  CNNs、GridNet和多尺度DenseNet。
convolutional neural fabrics和interlinked  CNNs这两个早期的工作缺乏对何时启动低分辨率并行流以及如何和在何处交换并行流信息的精心设计，并且不使用批量归一化和残差连接，因此表现并不令人满意。GridNet就像是多个U-Net的组合，包括两个对称信息交换阶段：第一阶段仅将信息从高分辨率传递给低分辨率，第二阶段仅将信息从低分辨率传递给高分辨率。 这限制了其分割质量。 多尺度DenseNet无法学习强大的高分辨率表示，因为没有从低分辨率表示中接收到任何信息。</p>
<h2 id="多尺度混合">多尺度混合</h2>
<p>多尺度混合被广泛的研究。比较直接的一种方式是把不同像素级别的图像丢入多个网络，同时聚合输出图像的信息。Hourglass、UNet和SegNet通过跳级链接混合了下采样和上采样过程中同一像素水平的特征信息。PSPNet和DeepLabV2混合了金字塔池化模式和atrous空间池化的信息。我们的多尺度池化模式包含了以上两种模式。但是不同在于：</p>
<ul>
<li>(1) 我们的混合输出是四个像素表达而不只是一个；</li>
<li>(2) 我们的混合模式被深度混合激励同时重复多次。</li>
</ul>
<h2 id="我们的方法">我们的方法</h2>
<p>我们的网络并行链接高像素到低像素的卷积流。它包括整个过程中的高像素表示和通过从多个像素级别中混合的对位置信息及其敏感的可靠的高像素表示。
这篇论文对我们之前的论文进行了充分的扩展和补充，我们另外添加了一些未发表文章的材料，以及应用到目标检测和实例分割框架中的结果。
与之前相比，技术的新颖之处在一下三个方面：</p>
<ul>
<li>(1) 与之前的HRNetV1相比，我们提出了HRNetV2和HRNetV2p，探索了四个像素级别的表示；</li>
<li>(2) 在多像素级别混合和通常的卷积之间创建了联系，这为探索HRNetV2和HRNetV2p在四个像素级别上的表示提供了支持；</li>
<li>(3) 我们显示了HRNetV2和HRNetV2p相对于HRNetV1的优越性，并介绍了HRNetV2和HRNetV2p在广泛的视觉问题中的应用，包括语义分割和目标检测。</li>
</ul>
<h1 id="三hrnet">三、HRNet</h1>
<p>我们将图像输入到一个枝干中，该枝干由两个步幅为2的3×3卷积组成，将分辨率降低到1/4，然后以相同的分辨率输出表示的主体（1/4）。主体如 <strong>图二</strong> 所示，下面进行了详细说明，它由几个组件组成：并行多分辨率卷积，重复的多分辨率融合和如 <strong>图四</strong> 所示的表示头。</p>
<h2 id="31-并行多分辨率卷积">3.1 并行多分辨率卷积</h2>
<p>我们从高分辨率卷积流作为第一步开始，逐步将高至低分辨率流逐个添加，形成新的阶段，然后并行连接多分辨率流。 结果，下一级并行流的分辨率包括前一级的分辨率，以及下一级的分辨率。</p>
<ul>
<li><strong>图二</strong>

<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20210412163023281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20210412163023281.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>
</li>
</ul>
<p><strong>图二</strong> 中展示了这个阶段的示意图，包含了四个并行流，逻辑如下图所示：


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20210412163100650.png" alt="在这里插入图片描述" id="20210412163100650" data-toggle="modal" data-target="#modal-20210412163100650"/>

  <div class="modal" id="modal-20210412163100650">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20210412163100650.png" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>

$N_{sr}$表示一个子流，$s$表示第$s$个stage，$r$表示属于的像素级别索引。第一个流的像素级别索引是$r$ = 1，第$r$个的分辨率是第一个流的分辨率的$\frac{1}{2^{r-1}}$。</p>
<h2 id="32--重复的多分辨率融合">3.2  重复的多分辨率融合</h2>
<p>这个混合模块的目的是交换多种分辨率表示的信息，它重复很多次（比如每四个残差单元重复一次）。
让我们看下<strong>图三</strong>中这个混合三个像素级别的例子：</p>
<ul>
<li><strong>图三</strong>：


<figure class="text-center">
  <img class="modal-trigger" src="https://img-blog.csdnimg.cn/20210412164537490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" id="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" data-toggle="modal" data-target="#modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70"/>

  <div class="modal" id="modal-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70">
    <div class="modal-dialog modal-lg modal-dialog-centered">
      <div class="modal-body">
        <img src="https://img-blog.csdnimg.cn/20210412164537490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA2MTYzNg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"/>
      </div>
  </div>
</figure>
</li>
</ul>



      </main>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Twitter" aria-label="Twitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://twitter.com/grpcio">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Google Groups" aria-label="Google Groups">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://groups.google.com/g/grpc-io">
      <i class="fab fa-google"></i>
    </a>
  </li>
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="Gitter" aria-label="Gitter">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://gitter.im/grpc/grpc">
      <i class="fab fa-gitter"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
        
        
<ul class="list-inline mb-0">
  
  <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="GitHub" aria-label="GitHub">
    <a class="text-white" target="_blank" rel="noopener noreferrer" href="https://github.com/grpc">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
</ul>

        
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">&copy; 2021 gRPC Authors
          
        </small>
        
        
      </div>
    </div>
    <div class="row text-center text-white small">
      <div class="col-12 text-center py-2 order-sm-2">
        <a href="https://www.linuxfoundation.org/terms" target="_blank" rel="noopener">Terms</a> |
        <a href="https://www.linuxfoundation.org/privacy" target="_blank" rel="noopener">Privacy</a> |
        <a href="https://www.linuxfoundation.org/trademark-usage" target="_blank" rel="noopener">Trademarks</a> |
        <a href="https://github.com/grpc/grpc.io/blob/main/LICENSE" target="_blank" rel="noopener">License</a> |
        <a href="/about/">About</a>
      </div>
    </div>
  </div>
</footer>


    </div>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>











<script src="/js/main.min.d862e8117d34e02dff1772aeeb47caf7ed851d0f8a3a5345f7a32ccfb2f6b87f.js" integrity="sha256-2GLoEX004C3/F3Ku60fK9&#43;2FHQ&#43;KOlNF96Msz7L2uH8=" crossorigin="anonymous"></script>




  </body>
</html>